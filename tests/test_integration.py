"""
MVPéªŒè¯ä¸ä¼˜åŒ–ä½“ç³»é›†æˆæµ‹è¯•

éªŒè¯æ‰€æœ‰æ¨¡å—çš„åŸºæœ¬åŠŸèƒ½å’Œé›†æˆæ€§
"""

import asyncio
import logging
import sys
import os
from pathlib import Path
from typing import Dict, Any, List
import tempfile
import shutil

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥æµ‹è¯•æ¨¡å—
from src.validation import MVPValidator, PerformanceAnalyzer, SystemChecker, BenchmarkRunner
from src.optimization import DatabaseOptimizer, CacheManager, MemoryOptimizer, ConfigTuner
from src.monitor import SystemMonitor, DiagnosticAnalyzer, AlertManager, ReportGenerator
from scripts import (
    MVPValidationRunner, HealthCheckScheduler, 
    PerformanceBenchmarkRunner, StressTestRunner
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(name)s: %(message)s')
logger = logging.getLogger(__name__)


class IntegrationTester:
    """é›†æˆæµ‹è¯•å™¨"""
    
    def __init__(self):
        # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºæµ‹è¯•
        self.temp_dir = Path(tempfile.mkdtemp(prefix='mvp_integration_test_'))
        
        # åŸºç¡€é…ç½®
        self.config = {
            'database': {
                'type': 'sqlite',
                'database': str(self.temp_dir / 'test.db')
            },
            'redis': {
                'host': 'localhost',
                'port': 6379,
                'db': 1  # ä½¿ç”¨æµ‹è¯•æ•°æ®åº“
            },
            'validation': {
                'technical_thresholds': {
                    'max_response_time_ms': 100,
                    'max_cpu_percent': 80,
                    'max_memory_percent': 70
                }
            },
            'stress_test': {
                'output_dir': str(self.temp_dir / 'stress_results')
            },
            'report_generator': {
                'output_dir': str(self.temp_dir / 'reports'),
                'chart_dir': str(self.temp_dir / 'charts')
            }
        }
        
        self.test_results = {}
        
    async def test_validation_module(self) -> bool:
        """æµ‹è¯•éªŒè¯æ¨¡å—"""
        logger.info("æµ‹è¯•éªŒè¯æ¨¡å—...")
        
        try:
            # æµ‹è¯•MVPéªŒè¯å™¨
            mvp_validator = MVPValidator(self.config)
            tech_results = await mvp_validator.validate_technical_metrics()
            assert 'results' in tech_results
            logger.info(f"MVPéªŒè¯å™¨æµ‹è¯•é€šè¿‡: {len(tech_results['results'])} ä¸ªæ£€æŸ¥é¡¹")
            
            # æµ‹è¯•æ€§èƒ½åˆ†æå™¨
            performance_analyzer = PerformanceAnalyzer(self.config)
            await performance_analyzer.start_monitoring()
            await asyncio.sleep(2)  # æ”¶é›†ä¸€äº›æ•°æ®
            current_metrics = performance_analyzer.get_current_metrics()
            await performance_analyzer.stop_monitoring()
            assert current_metrics is not None
            logger.info("æ€§èƒ½åˆ†æå™¨æµ‹è¯•é€šè¿‡")
            
            # æµ‹è¯•ç³»ç»Ÿæ£€æŸ¥å™¨
            system_checker = SystemChecker(self.config)
            health_results = await system_checker.run_health_checks()
            assert 'results' in health_results
            logger.info(f"ç³»ç»Ÿæ£€æŸ¥å™¨æµ‹è¯•é€šè¿‡: {len(health_results['results'])} ä¸ªæ£€æŸ¥é¡¹")
            
            # æµ‹è¯•åŸºå‡†æµ‹è¯•è¿è¡Œå™¨
            benchmark_runner = BenchmarkRunner(self.config)
            benchmark_results = await benchmark_runner.run_data_processing_benchmark()
            assert 'passed' in benchmark_results
            logger.info("åŸºå‡†æµ‹è¯•è¿è¡Œå™¨æµ‹è¯•é€šè¿‡")
            
            self.test_results['validation_module'] = True
            return True
            
        except Exception as e:
            logger.error(f"éªŒè¯æ¨¡å—æµ‹è¯•å¤±è´¥: {e}")
            self.test_results['validation_module'] = False
            return False
    
    async def test_optimization_module(self) -> bool:
        """æµ‹è¯•ä¼˜åŒ–æ¨¡å—"""
        logger.info("æµ‹è¯•ä¼˜åŒ–æ¨¡å—...")
        
        try:
            # æµ‹è¯•æ•°æ®åº“ä¼˜åŒ–å™¨
            db_optimizer = DatabaseOptimizer(self.config)
            db_suggestions = await db_optimizer.analyze_query_performance()
            assert isinstance(db_suggestions, list)
            logger.info(f"æ•°æ®åº“ä¼˜åŒ–å™¨æµ‹è¯•é€šè¿‡: {len(db_suggestions)} ä¸ªå»ºè®®")
            
            # æµ‹è¯•ç¼“å­˜ç®¡ç†å™¨
            cache_manager = CacheManager(self.config)
            await cache_manager.set('test_key', 'test_value')
            value = await cache_manager.get('test_key')
            assert value == 'test_value'
            cache_stats = cache_manager.get_stats()
            assert 'total_operations' in cache_stats
            logger.info("ç¼“å­˜ç®¡ç†å™¨æµ‹è¯•é€šè¿‡")
            
            # æµ‹è¯•å†…å­˜ä¼˜åŒ–å™¨
            memory_optimizer = MemoryOptimizer(self.config)
            memory_analysis = await memory_optimizer.analyze_memory_usage()
            assert 'current_usage' in memory_analysis
            logger.info("å†…å­˜ä¼˜åŒ–å™¨æµ‹è¯•é€šè¿‡")
            
            # æµ‹è¯•é…ç½®è°ƒä¼˜å™¨
            config_tuner = ConfigTuner(self.config)
            tuning_suggestions = await config_tuner.analyze_config()
            assert isinstance(tuning_suggestions, list)
            logger.info(f"é…ç½®è°ƒä¼˜å™¨æµ‹è¯•é€šè¿‡: {len(tuning_suggestions)} ä¸ªå»ºè®®")
            
            self.test_results['optimization_module'] = True
            return True
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–æ¨¡å—æµ‹è¯•å¤±è´¥: {e}")
            self.test_results['optimization_module'] = False
            return False
    
    async def test_monitoring_module(self) -> bool:
        """æµ‹è¯•ç›‘æ§æ¨¡å—"""
        logger.info("æµ‹è¯•ç›‘æ§æ¨¡å—...")
        
        try:
            # æµ‹è¯•ç³»ç»Ÿç›‘æ§å™¨
            system_monitor = SystemMonitor(self.config)
            await system_monitor.start_monitoring()
            await asyncio.sleep(3)  # æ”¶é›†ä¸€äº›æ•°æ®
            metrics = system_monitor.get_current_metrics()
            await system_monitor.stop_monitoring()
            assert metrics is not None
            logger.info("ç³»ç»Ÿç›‘æ§å™¨æµ‹è¯•é€šè¿‡")
            
            # æµ‹è¯•è¯Šæ–­åˆ†æå™¨
            diagnostic_analyzer = DiagnosticAnalyzer(self.config)
            test_metrics = [
                {'timestamp': '2023-01-01T00:00:00', 'cpu_percent': 95.0},
                {'timestamp': '2023-01-01T00:01:00', 'cpu_percent': 98.0},
                {'timestamp': '2023-01-01T00:02:00', 'cpu_percent': 96.0}
            ]
            anomalies = await diagnostic_analyzer.detect_anomalies('cpu_percent', test_metrics)
            assert isinstance(anomalies, list)
            logger.info(f"è¯Šæ–­åˆ†æå™¨æµ‹è¯•é€šè¿‡: æ£€æµ‹åˆ° {len(anomalies)} ä¸ªå¼‚å¸¸")
            
            # æµ‹è¯•å‘Šè­¦ç®¡ç†å™¨
            alert_manager = AlertManager(self.config)
            await alert_manager.start()
            await asyncio.sleep(1)
            await alert_manager.stop()
            stats = alert_manager.get_alert_statistics()
            assert 'total_rules' in stats
            logger.info("å‘Šè­¦ç®¡ç†å™¨æµ‹è¯•é€šè¿‡")
            
            # æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨
            report_generator = ReportGenerator(self.config)
            test_validation_results = {
                'technical_metrics': {
                    'results': [
                        {'name': 'æµ‹è¯•é¡¹1', 'passed': True},
                        {'name': 'æµ‹è¯•é¡¹2', 'passed': False}
                    ]
                }
            }
            report = await report_generator.generate_mvp_validation_report(test_validation_results)
            assert report.title == 'MVPéªŒè¯æŠ¥å‘Š'
            logger.info("æŠ¥å‘Šç”Ÿæˆå™¨æµ‹è¯•é€šè¿‡")
            
            self.test_results['monitoring_module'] = True
            return True
            
        except Exception as e:
            logger.error(f"ç›‘æ§æ¨¡å—æµ‹è¯•å¤±è´¥: {e}")
            self.test_results['monitoring_module'] = False
            return False
    
    async def test_scripts_module(self) -> bool:
        """æµ‹è¯•è„šæœ¬æ¨¡å—"""
        logger.info("æµ‹è¯•è„šæœ¬æ¨¡å—...")
        
        try:
            # æµ‹è¯•MVPéªŒè¯è¿è¡Œå™¨
            mvp_runner = MVPValidationRunner(self.config)
            validation_results = await mvp_runner.run_technical_validation()
            assert 'results' in validation_results
            logger.info("MVPéªŒè¯è¿è¡Œå™¨æµ‹è¯•é€šè¿‡")
            
            # æµ‹è¯•å¥åº·æ£€æŸ¥è°ƒåº¦å™¨
            health_scheduler = HealthCheckScheduler(self.config)
            health_result = await health_scheduler.run_health_check()
            assert 'timestamp' in health_result
            logger.info("å¥åº·æ£€æŸ¥è°ƒåº¦å™¨æµ‹è¯•é€šè¿‡")
            
            # æµ‹è¯•æ€§èƒ½åŸºå‡†æµ‹è¯•è¿è¡Œå™¨
            benchmark_runner = PerformanceBenchmarkRunner(self.config)
            benchmark_results = await benchmark_runner.run_basic_benchmarks()
            assert isinstance(benchmark_results, list)
            logger.info(f"æ€§èƒ½åŸºå‡†æµ‹è¯•è¿è¡Œå™¨æµ‹è¯•é€šè¿‡: {len(benchmark_results)} ä¸ªåŸºå‡†æµ‹è¯•")
            
            # æµ‹è¯•å‹åŠ›æµ‹è¯•è¿è¡Œå™¨
            stress_runner = StressTestRunner(self.config)
            from scripts.stress_test_runner import StressTestConfig
            
            test_config = StressTestConfig(
                test_name="é›†æˆæµ‹è¯•CPUæµ‹è¯•",
                test_type="cpu",
                duration_seconds=10,  # çŸ­æ—¶é—´æµ‹è¯•
                target_load=50.0,
                concurrent_workers=2
            )
            
            stress_result = await stress_runner.run_stress_test(test_config)
            assert stress_result.test_name == "é›†æˆæµ‹è¯•CPUæµ‹è¯•"
            logger.info("å‹åŠ›æµ‹è¯•è¿è¡Œå™¨æµ‹è¯•é€šè¿‡")
            
            # æ¸…ç†èµ„æº
            stress_runner.cleanup()
            
            self.test_results['scripts_module'] = True
            return True
            
        except Exception as e:
            logger.error(f"è„šæœ¬æ¨¡å—æµ‹è¯•å¤±è´¥: {e}")
            self.test_results['scripts_module'] = False
            return False
    
    async def test_integration(self) -> bool:
        """æµ‹è¯•æ¨¡å—é—´é›†æˆ"""
        logger.info("æµ‹è¯•æ¨¡å—é—´é›†æˆ...")
        
        try:
            # é›†æˆæµ‹è¯•ï¼šéªŒè¯ -> ä¼˜åŒ– -> ç›‘æ§ -> æŠ¥å‘Š
            
            # 1. è¿è¡ŒéªŒè¯
            mvp_validator = MVPValidator(self.config)
            validation_results = await mvp_validator.validate_technical_metrics()
            
            # 2. åŸºäºéªŒè¯ç»“æœè¿›è¡Œä¼˜åŒ–
            if validation_results.get('overall_passed', True):
                logger.info("éªŒè¯é€šè¿‡ï¼Œè¿›è¡Œä¼˜åŒ–...")
                cache_manager = CacheManager(self.config)
                await cache_manager.set('validation_status', 'passed')
                optimization_status = await cache_manager.get('validation_status')
                assert optimization_status == 'passed'
            
            # 3. ç›‘æ§ä¼˜åŒ–è¿‡ç¨‹
            system_monitor = SystemMonitor(self.config)
            await system_monitor.start_monitoring()
            await asyncio.sleep(2)
            metrics = system_monitor.get_current_metrics()
            await system_monitor.stop_monitoring()
            
            # 4. ç”Ÿæˆé›†æˆæŠ¥å‘Š
            report_generator = ReportGenerator(self.config)
            integrated_results = {
                'validation': validation_results,
                'monitoring': {'metrics': metrics.to_dict() if metrics else {}}
            }
            
            # æ¨¡æ‹ŸæŠ¥å‘Šç”Ÿæˆ
            report_data = {
                'technical_metrics': {
                    'results': [
                        {'name': 'é›†æˆæµ‹è¯•é¡¹', 'passed': True}
                    ]
                }
            }
            
            report = await report_generator.generate_mvp_validation_report(report_data)
            
            logger.info("æ¨¡å—é—´é›†æˆæµ‹è¯•é€šè¿‡")
            self.test_results['integration'] = True
            return True
            
        except Exception as e:
            logger.error(f"æ¨¡å—é—´é›†æˆæµ‹è¯•å¤±è´¥: {e}")
            self.test_results['integration'] = False
            return False
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰é›†æˆæµ‹è¯•"""
        logger.info("å¼€å§‹è¿è¡ŒMVPéªŒè¯ä¸ä¼˜åŒ–ä½“ç³»é›†æˆæµ‹è¯•...")
        
        test_functions = [
            ('validation_module', self.test_validation_module),
            ('optimization_module', self.test_optimization_module),
            ('monitoring_module', self.test_monitoring_module),
            ('scripts_module', self.test_scripts_module),
            ('integration', self.test_integration)
        ]
        
        results = {}
        passed_tests = 0
        total_tests = len(test_functions)
        
        for test_name, test_func in test_functions:
            logger.info(f"è¿è¡Œæµ‹è¯•: {test_name}")
            try:
                success = await test_func()
                results[test_name] = success
                if success:
                    passed_tests += 1
                    logger.info(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
                else:
                    logger.error(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
            except Exception as e:
                logger.error(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
                results[test_name] = False
        
        # è®¡ç®—æ€»ä½“ç»“æœ
        pass_rate = (passed_tests / total_tests) * 100
        overall_passed = passed_tests == total_tests
        
        summary = {
            'overall_passed': overall_passed,
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'failed_tests': total_tests - passed_tests,
            'pass_rate': pass_rate,
            'detailed_results': results
        }
        
        logger.info("=" * 60)
        logger.info("é›†æˆæµ‹è¯•å®Œæˆ!")
        logger.info(f"æ€»æµ‹è¯•æ•°: {total_tests}")
        logger.info(f"é€šè¿‡æµ‹è¯•: {passed_tests}")
        logger.info(f"å¤±è´¥æµ‹è¯•: {total_tests - passed_tests}")
        logger.info(f"é€šè¿‡ç‡: {pass_rate:.1f}%")
        logger.info(f"æ•´ä½“ç»“æœ: {'âœ… é€šè¿‡' if overall_passed else 'âŒ å¤±è´¥'}")
        
        return summary
    
    def cleanup(self):
        """æ¸…ç†æµ‹è¯•èµ„æº"""
        try:
            if self.temp_dir.exists():
                shutil.rmtree(self.temp_dir)
                logger.info(f"å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {self.temp_dir}")
        except Exception as e:
            logger.warning(f"æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    tester = IntegrationTester()
    
    try:
        results = await tester.run_all_tests()
        
        # è¾“å‡ºæœ€ç»ˆç»“æœ
        if results['overall_passed']:
            print("\nğŸ‰ MVPéªŒè¯ä¸ä¼˜åŒ–ä½“ç³»é›†æˆæµ‹è¯•å…¨éƒ¨é€šè¿‡!")
            print("ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥æŠ•å…¥ä½¿ç”¨ã€‚")
            return 0
        else:
            print(f"\nâš ï¸  é›†æˆæµ‹è¯•éƒ¨åˆ†å¤±è´¥ (é€šè¿‡ç‡: {results['pass_rate']:.1f}%)")
            print("è¯·æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•é¡¹å¹¶ä¿®å¤ç›¸å…³é—®é¢˜ã€‚")
            return 1
            
    except Exception as e:
        logger.error(f"é›†æˆæµ‹è¯•è¿è¡Œå¼‚å¸¸: {e}")
        print(f"\nâŒ é›†æˆæµ‹è¯•è¿è¡Œå¤±è´¥: {e}")
        return 1
        
    finally:
        tester.cleanup()


if __name__ == '__main__':
    import sys
    exit_code = asyncio.run(main())
    sys.exit(exit_code)