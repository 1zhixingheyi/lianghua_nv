# é‡åŒ–äº¤æ˜“ç³»ç»Ÿç›‘æ§è¿ç»´æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»é‡åŒ–äº¤æ˜“ç³»ç»Ÿçš„ç›‘æ§è¿ç»´æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ç³»ç»Ÿç›‘æ§ã€æ€§èƒ½ç›‘æ§ã€ä¸šåŠ¡ç›‘æ§ã€å‘Šè­¦æœºåˆ¶ã€æ—¥å¿—ç®¡ç†ã€æ•…éšœå¤„ç†ç­‰å†…å®¹ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šå¯é è¿è¡Œã€‚

## ğŸ¯ ç›‘æ§ä½“ç³»æ¶æ„

### ç›‘æ§å±‚æ¬¡ç»“æ„

```
ç›‘æ§ä½“ç³»
â”œâ”€â”€ åŸºç¡€è®¾æ–½ç›‘æ§
â”‚   â”œâ”€â”€ æœåŠ¡å™¨èµ„æºç›‘æ§
â”‚   â”œâ”€â”€ ç½‘ç»œç›‘æ§
â”‚   â””â”€â”€ å­˜å‚¨ç›‘æ§
â”œâ”€â”€ ä¸­é—´ä»¶ç›‘æ§
â”‚   â”œâ”€â”€ æ•°æ®åº“ç›‘æ§
â”‚   â”œâ”€â”€ Redisç›‘æ§
â”‚   â””â”€â”€ æ¶ˆæ¯é˜Ÿåˆ—ç›‘æ§
â”œâ”€â”€ åº”ç”¨ç›‘æ§
â”‚   â”œâ”€â”€ æœåŠ¡å¯ç”¨æ€§ç›‘æ§
â”‚   â”œâ”€â”€ APIæ€§èƒ½ç›‘æ§
â”‚   â””â”€â”€ ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§
â””â”€â”€ ä¸šåŠ¡ç›‘æ§
    â”œâ”€â”€ äº¤æ˜“ç›‘æ§
    â”œâ”€â”€ ç­–ç•¥ç›‘æ§
    â””â”€â”€ é£æ§ç›‘æ§
```

### ç›‘æ§æŠ€æœ¯æ ˆ

- **æ•°æ®æ”¶é›†**: Prometheus + Node Exporter + Custom Exporters
- **æ•°æ®å­˜å‚¨**: Prometheus + InfluxDB
- **å¯è§†åŒ–**: Grafana + Custom Dashboard
- **å‘Šè­¦**: AlertManager + é’‰é’‰/é‚®ä»¶/çŸ­ä¿¡
- **æ—¥å¿—**: ELK Stack (Elasticsearch + Logstash + Kibana)
- **é“¾è·¯è¿½è¸ª**: Jaeger
- **æ€§èƒ½ç›‘æ§**: APM (Application Performance Monitoring)

## ğŸ”§ ç›‘æ§ç³»ç»Ÿéƒ¨ç½²

### 1. Prometheusé…ç½®

#### å®‰è£…å’Œé…ç½®

```yaml
# docker-compose.monitoring.yml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    restart: unless-stopped

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    restart: unless-stopped

  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./config/alertmanager.yml:/etc/alertmanager/alertmanager.yml
    restart: unless-stopped

volumes:
  prometheus_data:
```

#### Prometheusé…ç½®æ–‡ä»¶

```yaml
# config/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "rules/*.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  # ç³»ç»Ÿç›‘æ§
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  # åº”ç”¨ç›‘æ§
  - job_name: 'lianghua-app'
    static_configs:
      - targets: ['app:5000']
    metrics_path: '/metrics'
    scrape_interval: 10s

  # MySQLç›‘æ§
  - job_name: 'mysql'
    static_configs:
      - targets: ['mysql-exporter:9104']

  # Redisç›‘æ§
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']

  # Nginxç›‘æ§
  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx-exporter:9113']
```

### 2. è‡ªå®šä¹‰åº”ç”¨ç›‘æ§

#### Flaskåº”ç”¨æŒ‡æ ‡æš´éœ²

```python
# monitor/metrics.py
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from flask import Response
import time
import psutil
import functools

# å®šä¹‰æŒ‡æ ‡
REQUEST_COUNT = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint', 'status'])
REQUEST_LATENCY = Histogram('http_request_duration_seconds', 'HTTP request latency')
ACTIVE_CONNECTIONS = Gauge('active_connections_total', 'Active WebSocket connections')
STRATEGY_PERFORMANCE = Gauge('strategy_performance', 'Strategy performance metrics', ['strategy_name', 'metric_type'])
ORDER_COUNT = Counter('orders_total', 'Total orders placed', ['symbol', 'side', 'status'])
PORTFOLIO_VALUE = Gauge('portfolio_value_total', 'Total portfolio value')
SYSTEM_CPU = Gauge('system_cpu_percent', 'System CPU usage percentage')
SYSTEM_MEMORY = Gauge('system_memory_percent', 'System memory usage percentage')

class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self):
        self.start_time = time.time()
        
    def track_request(self, method, endpoint, status_code, duration):
        """è®°å½•HTTPè¯·æ±‚æŒ‡æ ‡"""
        REQUEST_COUNT.labels(method=method, endpoint=endpoint, status=status_code).inc()
        REQUEST_LATENCY.observe(duration)
        
    def track_order(self, symbol, side, status):
        """è®°å½•è®¢å•æŒ‡æ ‡"""
        ORDER_COUNT.labels(symbol=symbol, side=side, status=status).inc()
        
    def update_portfolio_value(self, value):
        """æ›´æ–°æŠ•èµ„ç»„åˆä»·å€¼"""
        PORTFOLIO_VALUE.set(value)
        
    def update_strategy_performance(self, strategy_name, metric_type, value):
        """æ›´æ–°ç­–ç•¥æ€§èƒ½æŒ‡æ ‡"""
        STRATEGY_PERFORMANCE.labels(strategy_name=strategy_name, metric_type=metric_type).set(value)
        
    def update_system_metrics(self):
        """æ›´æ–°ç³»ç»ŸæŒ‡æ ‡"""
        SYSTEM_CPU.set(psutil.cpu_percent())
        SYSTEM_MEMORY.set(psutil.virtual_memory().percent)
        
    def track_websocket_connection(self, increment=True):
        """è·Ÿè¸ªWebSocketè¿æ¥"""
        if increment:
            ACTIVE_CONNECTIONS.inc()
        else:
            ACTIVE_CONNECTIONS.dec()

# å…¨å±€æŒ‡æ ‡æ”¶é›†å™¨å®ä¾‹
metrics_collector = MetricsCollector()

def monitor_requests(f):
    """è¯·æ±‚ç›‘æ§è£…é¥°å™¨"""
    @functools.wraps(f)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        
        try:
            response = f(*args, **kwargs)
            status_code = getattr(response, 'status_code', 200)
        except Exception as e:
            status_code = 500
            raise
        finally:
            duration = time.time() - start_time
            from flask import request
            metrics_collector.track_request(
                request.method,
                request.endpoint or 'unknown',
                status_code,
                duration
            )
            
        return response
    return wrapper

@app.route('/metrics')
def metrics():
    """PrometheusæŒ‡æ ‡ç«¯ç‚¹"""
    # æ›´æ–°ç³»ç»ŸæŒ‡æ ‡
    metrics_collector.update_system_metrics()
    
    # ç”ŸæˆPrometheusæ ¼å¼çš„æŒ‡æ ‡
    return Response(generate_latest(), mimetype='text/plain')
```

#### ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§

```python
# monitor/business_metrics.py
import threading
import time
from datetime import datetime, timedelta
from data.database import get_database_manager
from .metrics import metrics_collector

class BusinessMetricsCollector:
    """ä¸šåŠ¡æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self):
        self.db = get_database_manager()
        self.running = False
        self.collection_thread = None
        
    def start_collection(self):
        """å¯åŠ¨æŒ‡æ ‡æ”¶é›†"""
        if self.running:
            return
            
        self.running = True
        self.collection_thread = threading.Thread(target=self._collection_loop)
        self.collection_thread.daemon = True
        self.collection_thread.start()
        
    def stop_collection(self):
        """åœæ­¢æŒ‡æ ‡æ”¶é›†"""
        self.running = False
        if self.collection_thread:
            self.collection_thread.join()
            
    def _collection_loop(self):
        """æŒ‡æ ‡æ”¶é›†å¾ªç¯"""
        while self.running:
            try:
                # æ”¶é›†ä¸šåŠ¡æŒ‡æ ‡
                self._collect_portfolio_metrics()
                self._collect_strategy_metrics()
                self._collect_trading_metrics()
                self._collect_risk_metrics()
                
                # æ¯30ç§’æ”¶é›†ä¸€æ¬¡
                time.sleep(30)
                
            except Exception as e:
                print(f"ä¸šåŠ¡æŒ‡æ ‡æ”¶é›†é”™è¯¯: {e}")
                time.sleep(60)
                
    def _collect_portfolio_metrics(self):
        """æ”¶é›†æŠ•èµ„ç»„åˆæŒ‡æ ‡"""
        try:
            # è·å–å½“å‰æŠ•èµ„ç»„åˆä»·å€¼
            portfolio_value = self._get_portfolio_value()
            metrics_collector.update_portfolio_value(portfolio_value)
            
            # è®¡ç®—æ—¥æ”¶ç›Šç‡
            daily_return = self._calculate_daily_return()
            metrics_collector.update_strategy_performance('portfolio', 'daily_return', daily_return)
            
        except Exception as e:
            print(f"æŠ•èµ„ç»„åˆæŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")
            
    def _collect_strategy_metrics(self):
        """æ”¶é›†ç­–ç•¥æŒ‡æ ‡"""
        try:
            strategies = self._get_active_strategies()
            
            for strategy in strategies:
                # ç­–ç•¥æ”¶ç›Šç‡
                returns = self._calculate_strategy_returns(strategy['id'])
                metrics_collector.update_strategy_performance(
                    strategy['name'], 'total_return', returns
                )
                
                # ç­–ç•¥ä»“ä½
                position_ratio = self._get_strategy_position_ratio(strategy['id'])
                metrics_collector.update_strategy_performance(
                    strategy['name'], 'position_ratio', position_ratio
                )
                
                # ç­–ç•¥çŠ¶æ€
                status = 1 if strategy['status'] == 'active' else 0
                metrics_collector.update_strategy_performance(
                    strategy['name'], 'status', status
                )
                
        except Exception as e:
            print(f"ç­–ç•¥æŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")
            
    def _collect_trading_metrics(self):
        """æ”¶é›†äº¤æ˜“æŒ‡æ ‡"""
        try:
            today = datetime.now().date()
            
            # ä»Šæ—¥è®¢å•ç»Ÿè®¡
            order_stats = self._get_daily_order_stats(today)
            for symbol, stats in order_stats.items():
                for side, count in stats.items():
                    # è¿™é‡Œåªæ›´æ–°è®¡æ•°ï¼Œå®é™…è®¢å•åœ¨ä¸‹å•æ—¶è®°å½•
                    pass
                    
            # æˆäº¤ç»Ÿè®¡
            execution_stats = self._get_execution_stats(today)
            metrics_collector.update_strategy_performance(
                'trading', 'execution_rate', execution_stats['execution_rate']
            )
            
        except Exception as e:
            print(f"äº¤æ˜“æŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")
            
    def _collect_risk_metrics(self):
        """æ”¶é›†é£æ§æŒ‡æ ‡"""
        try:
            # å½“å‰å›æ’¤
            current_drawdown = self._calculate_current_drawdown()
            metrics_collector.update_strategy_performance('risk', 'current_drawdown', current_drawdown)
            
            # ä»“ä½é›†ä¸­åº¦
            position_concentration = self._calculate_position_concentration()
            metrics_collector.update_strategy_performance('risk', 'position_concentration', position_concentration)
            
            # é£é™©é¢„ç®—ä½¿ç”¨ç‡
            risk_budget_usage = self._calculate_risk_budget_usage()
            metrics_collector.update_strategy_performance('risk', 'risk_budget_usage', risk_budget_usage)
            
        except Exception as e:
            print(f"é£æ§æŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")
            
    def _get_portfolio_value(self) -> float:
        """è·å–æŠ•èµ„ç»„åˆä»·å€¼"""
        # å®ç°æŠ•èµ„ç»„åˆä»·å€¼è®¡ç®—
        return 1000000.0  # ç¤ºä¾‹å€¼
        
    def _calculate_daily_return(self) -> float:
        """è®¡ç®—æ—¥æ”¶ç›Šç‡"""
        # å®ç°æ—¥æ”¶ç›Šç‡è®¡ç®—
        return 0.01  # ç¤ºä¾‹å€¼
        
    def _get_active_strategies(self) -> list:
        """è·å–æ´»è·ƒç­–ç•¥åˆ—è¡¨"""
        # ä»æ•°æ®åº“è·å–æ´»è·ƒç­–ç•¥
        return [
            {'id': 1, 'name': 'RSI_Strategy', 'status': 'active'},
            {'id': 2, 'name': 'MACD_Strategy', 'status': 'active'}
        ]
        
    def _calculate_strategy_returns(self, strategy_id: int) -> float:
        """è®¡ç®—ç­–ç•¥æ”¶ç›Šç‡"""
        # å®ç°ç­–ç•¥æ”¶ç›Šç‡è®¡ç®—
        return 0.05  # ç¤ºä¾‹å€¼
        
    def _get_strategy_position_ratio(self, strategy_id: int) -> float:
        """è·å–ç­–ç•¥ä»“ä½æ¯”ä¾‹"""
        # å®ç°ä»“ä½æ¯”ä¾‹è®¡ç®—
        return 0.8  # ç¤ºä¾‹å€¼
        
    def _get_daily_order_stats(self, date) -> dict:
        """è·å–æ—¥è®¢å•ç»Ÿè®¡"""
        # å®ç°è®¢å•ç»Ÿè®¡
        return {}
        
    def _get_execution_stats(self, date) -> dict:
        """è·å–æˆäº¤ç»Ÿè®¡"""
        # å®ç°æˆäº¤ç»Ÿè®¡
        return {'execution_rate': 0.95}
        
    def _calculate_current_drawdown(self) -> float:
        """è®¡ç®—å½“å‰å›æ’¤"""
        # å®ç°å›æ’¤è®¡ç®—
        return -0.05  # ç¤ºä¾‹å€¼
        
    def _calculate_position_concentration(self) -> float:
        """è®¡ç®—ä»“ä½é›†ä¸­åº¦"""
        # å®ç°é›†ä¸­åº¦è®¡ç®—
        return 0.3  # ç¤ºä¾‹å€¼
        
    def _calculate_risk_budget_usage(self) -> float:
        """è®¡ç®—é£é™©é¢„ç®—ä½¿ç”¨ç‡"""
        # å®ç°é£é™©é¢„ç®—è®¡ç®—
        return 0.6  # ç¤ºä¾‹å€¼

# å…¨å±€ä¸šåŠ¡æŒ‡æ ‡æ”¶é›†å™¨
business_metrics = BusinessMetricsCollector()
```

### 3. å‘Šè­¦è§„åˆ™é…ç½®

#### Prometheuså‘Šè­¦è§„åˆ™

```yaml
# config/rules/system_alerts.yml
groups:
  - name: system_alerts
    rules:
      # ç³»ç»Ÿèµ„æºå‘Šè­¦
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is above 80% for more than 5 minutes"

      - alert: HighMemoryUsage
        expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Available memory is below 10%"

      - alert: DiskSpaceLow
        expr: node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"} * 100 < 10
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Low disk space on {{ $labels.instance }}"
          description: "Disk space is below 10%"

  - name: application_alerts
    rules:
      # åº”ç”¨æœåŠ¡å‘Šè­¦
      - alert: ApplicationDown
        expr: up{job="lianghua-app"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Application is down"
          description: "Lianghua application has been down for more than 1 minute"

      - alert: HighResponseTime
        expr: histogram_quantile(0.95, http_request_duration_seconds_bucket) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High response time"
          description: "95th percentile response time is above 5 seconds"

      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate"
          description: "Error rate is above 10%"

  - name: business_alerts
    rules:
      # ä¸šåŠ¡å‘Šè­¦
      - alert: HighDrawdown
        expr: strategy_performance{metric_type="current_drawdown"} < -0.15
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "High portfolio drawdown"
          description: "Portfolio drawdown exceeds 15%"

      - alert: StrategyDown
        expr: strategy_performance{metric_type="status"} == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Strategy is inactive"
          description: "Strategy {{ $labels.strategy_name }} has been inactive for 5 minutes"

      - alert: LowExecutionRate
        expr: strategy_performance{metric_type="execution_rate"} < 0.8
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Low order execution rate"
          description: "Order execution rate is below 80%"
```

#### AlertManageré…ç½®

```yaml
# config/alertmanager.yml
global:
  smtp_smarthost: 'smtp.example.com:587'
  smtp_from: 'alerts@lianghua.com'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'default'
  routes:
    - match:
        severity: critical
      receiver: 'critical-alerts'
    - match:
        severity: warning
      receiver: 'warning-alerts'

receivers:
  - name: 'default'
    webhook_configs:
      - url: 'http://localhost:5000/webhook/alerts'

  - name: 'critical-alerts'
    email_configs:
      - to: 'admin@lianghua.com'
        subject: 'CRITICAL: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Labels: {{ range .Labels.SortedPairs }}{{ .Name }}={{ .Value }} {{ end }}
          {{ end }}
    webhook_configs:
      - url: 'http://localhost:5000/webhook/critical'

  - name: 'warning-alerts'
    webhook_configs:
      - url: 'http://localhost:5000/webhook/warning'

inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'instance']
```

### 4. Grafanaä»ªè¡¨æ¿

#### ç³»ç»Ÿç›‘æ§ä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "id": null,
    "title": "é‡åŒ–äº¤æ˜“ç³»ç»Ÿç›‘æ§",
    "tags": ["lianghua", "monitoring"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "ç³»ç»Ÿæ¦‚è§ˆ",
        "type": "stat",
        "targets": [
          {
            "expr": "up{job=\"lianghua-app\"}",
            "legendFormat": "åº”ç”¨çŠ¶æ€"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "thresholds"
            },
            "thresholds": {
              "steps": [
                {"color": "red", "value": 0},
                {"color": "green", "value": 1}
              ]
            }
          }
        }
      },
      {
        "id": 2,
        "title": "CPUä½¿ç”¨ç‡",
        "type": "graph",
        "targets": [
          {
            "expr": "100 - (avg(irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "CPUä½¿ç”¨ç‡"
          }
        ],
        "yAxes": [
          {
            "min": 0,
            "max": 100,
            "unit": "percent"
          }
        ]
      },
      {
        "id": 3,
        "title": "å†…å­˜ä½¿ç”¨ç‡",
        "type": "graph",
        "targets": [
          {
            "expr": "(1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100",
            "legendFormat": "å†…å­˜ä½¿ç”¨ç‡"
          }
        ]
      },
      {
        "id": 4,
        "title": "HTTPè¯·æ±‚é‡",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{endpoint}}"
          }
        ]
      },
      {
        "id": 5,
        "title": "å“åº”æ—¶é—´",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, http_request_duration_seconds_bucket)",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, http_request_duration_seconds_bucket)",
            "legendFormat": "50th percentile"
          }
        ]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "30s"
  }
}
```

#### ä¸šåŠ¡ç›‘æ§ä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "title": "ä¸šåŠ¡ç›‘æ§ä»ªè¡¨æ¿",
    "panels": [
      {
        "id": 1,
        "title": "æŠ•èµ„ç»„åˆä»·å€¼",
        "type": "stat",
        "targets": [
          {
            "expr": "portfolio_value_total",
            "legendFormat": "æ€»ä»·å€¼"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "currency",
            "decimals": 2
          }
        }
      },
      {
        "id": 2,
        "title": "ç­–ç•¥æ”¶ç›Šç‡",
        "type": "graph",
        "targets": [
          {
            "expr": "strategy_performance{metric_type=\"total_return\"}",
            "legendFormat": "{{strategy_name}}"
          }
        ]
      },
      {
        "id": 3,
        "title": "å½“å‰å›æ’¤",
        "type": "gauge",
        "targets": [
          {
            "expr": "strategy_performance{metric_type=\"current_drawdown\"}",
            "legendFormat": "å›æ’¤"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "min": -50,
            "max": 0,
            "thresholds": {
              "steps": [
                {"color": "green", "value": -5},
                {"color": "yellow", "value": -10},
                {"color": "red", "value": -20}
              ]
            }
          }
        }
      },
      {
        "id": 4,
        "title": "è®¢å•ç»Ÿè®¡",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(orders_total[5m])",
            "legendFormat": "{{side}} orders"
          }
        ]
      },
      {
        "id": 5,
        "title": "ç­–ç•¥çŠ¶æ€",
        "type": "table",
        "targets": [
          {
            "expr": "strategy_performance{metric_type=\"status\"}",
            "format": "table"
          }
        ]
      }
    ]
  }
}
```

## ğŸ“Š æ—¥å¿—ç®¡ç†

### 1. æ—¥å¿—æ”¶é›†é…ç½®

#### ELK Stackéƒ¨ç½²

```yaml
# docker-compose.elk.yml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.14.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

  logstash:
    image: docker.elastic.co/logstash/logstash:7.14.0
    container_name: logstash
    ports:
      - "5044:5044"
    volumes:
      - ./config/logstash/pipeline:/usr/share/logstash/pipeline
      - ./config/logstash/config:/usr/share/logstash/config
    depends_on:
      - elasticsearch

  kibana:
    image: docker.elastic.co/kibana/kibana:7.14.0
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch

  filebeat:
    image: docker.elastic.co/beats/filebeat:7.14.0
    container_name: filebeat
    volumes:
      - ./config/filebeat.yml:/usr/share/filebeat/filebeat.yml
      - /var/log:/var/log:ro
      - ./logs:/app/logs:ro
    depends_on:
      - logstash

volumes:
  elasticsearch_data:
```

#### Filebeaté…ç½®

```yaml
# config/filebeat.yml
filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /app/logs/*.log
    fields:
      service: lianghua-trading
      environment: production
    fields_under_root: true
    multiline.pattern: '^\d{4}-\d{2}-\d{2}'
    multiline.negate: true
    multiline.match: after

  - type: log
    enabled: true
    paths:
      - /var/log/nginx/access.log
    fields:
      service: nginx
      log_type: access

  - type: log
    enabled: true
    paths:
      - /var/log/nginx/error.log
    fields:
      service: nginx
      log_type: error

output.logstash:
  hosts: ["logstash:5044"]

processors:
  - add_host_metadata:
      when.not.contains.tags: forwarded
```

#### Logstashé…ç½®

```ruby
# config/logstash/pipeline/logstash.conf
input {
  beats {
    port => 5044
  }
}

filter {
  if [service] == "lianghua-trading" {
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:timestamp} - %{WORD:logger} - %{WORD:level} - %{GREEDYDATA:log_message}" 
      }
    }
    
    date {
      match => [ "timestamp", "yyyy-MM-dd HH:mm:ss,SSS" ]
    }
    
    if [level] == "ERROR" {
      mutate {
        add_tag => [ "error" ]
      }
    }
  }
  
  if [service] == "nginx" and [log_type] == "access" {
    grok {
      match => { 
        "message" => "%{NGINXACCESS}" 
      }
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "%{service}-%{+YYYY.MM.dd}"
  }
  
  # é”™è¯¯æ—¥å¿—åŒæ—¶è¾“å‡ºåˆ°å‘Šè­¦
  if "error" in [tags] {
    http {
      url => "http://alertmanager:9093/api/v1/alerts"
      http_method => "post"
      format => "json"
      mapping => {
        "alerts" => [
          {
            "labels" => {
              "alertname" => "ApplicationError"
              "service" => "%{service}"
              "severity" => "warning"
            }
            "annotations" => {
              "summary" => "Application error detected"
              "description" => "%{log_message}"
            }
          }
        ]
      }
    }
  }
}
```

### 2. ç»“æ„åŒ–æ—¥å¿—

#### åº”ç”¨æ—¥å¿—æ ¼å¼åŒ–

```python
# utils/logging_config.py
import logging
import json
from datetime import datetime
from typing import Dict, Any

class JSONFormatter(logging.Formatter):
    """JSONæ ¼å¼çš„æ—¥å¿—æ ¼å¼åŒ–å™¨"""
    
    def format(self, record):
        log_entry = {
            'timestamp': datetime.fromtimestamp(record.created).isoformat(),
            'level': record.levelname,
            'logger': record.name,
            'message': record.getMessage(),
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno
        }
        
        # æ·»åŠ é¢å¤–å­—æ®µ
        if hasattr(record, 'user_id'):
            log_entry['user_id'] = record.user_id
        if hasattr(record, 'request_id'):
            log_entry['request_id'] = record.request_id
        if hasattr(record, 'strategy_id'):
            log_entry['strategy_id'] = record.strategy_id
            
        # å¼‚å¸¸ä¿¡æ¯
        if record.exc_info:
            log_entry['exception'] = self.formatException(record.exc_info)
            
        return json.dumps(log_entry, ensure_ascii=False)

class ContextFilter(logging.Filter):
    """ä¸Šä¸‹æ–‡è¿‡æ»¤å™¨ï¼Œæ·»åŠ è¯·æ±‚IDç­‰ä¿¡æ¯"""
    
    def filter(self, record):
        # ä»Flaskä¸Šä¸‹æ–‡è·å–è¯·æ±‚ID
        try:
            from flask import g
            if hasattr(g, 'request_id'):
                record.request_id = g.request_id
        except:
            pass
            
        return True

def setup_logging(app_name: str, log_level: str = 'INFO'):
    """è®¾ç½®åº”ç”¨æ—¥å¿—"""
    
    # åˆ›å»ºæ ¹æ—¥å¿—å™¨
    logger = logging.getLogger(app_name)
    logger.setLevel(getattr(logging, log_level.upper()))
    
    # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
    logger.handlers.clear()
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(JSONFormatter())
    console_handler.addFilter(ContextFilter())
    logger.addHandler(console_handler)
    
    # æ–‡ä»¶å¤„ç†å™¨
    from logging.handlers import RotatingFileHandler
    file_handler = RotatingFileHandler(
        f'logs/{app_name}.log',
        maxBytes=10*1024*1024,  # 10MB
        backupCount=5
    )
    file_handler.setFormatter(JSONFormatter())
    file_handler.addFilter(ContextFilter())
    logger.addHandler(file_handler)
    
    # é”™è¯¯æ–‡ä»¶å¤„ç†å™¨
    error_handler = RotatingFileHandler(
        f'logs/{app_name}-error.log',
        maxBytes=10*1024*1024,
        backupCount=5
    )
    error_handler.setLevel(logging.ERROR)
    error_handler.setFormatter(JSONFormatter())
    error_handler.addFilter(ContextFilter())
    logger.addHandler(error_handler)
    
    return logger

# ä¸šåŠ¡æ—¥å¿—è®°å½•å™¨
def log_business_event(event_type: str, details: Dict[str, Any], logger: logging.Logger):
    """è®°å½•ä¸šåŠ¡äº‹ä»¶"""
    log_data = {
        'event_type': event_type,
        'details': details,
        'timestamp': datetime.now().isoformat()
    }
    
    logger.info(f"BUSINESS_EVENT: {json.dumps(log_data)}")

def log_trading_event(action: str, symbol: str, quantity: int, price: float, 
                     strategy: str, logger: logging.Logger):
    """è®°å½•äº¤æ˜“äº‹ä»¶"""
    trading_data = {
        'action': action,
        'symbol': symbol,
        'quantity': quantity,
        'price': price,
        'strategy': strategy,
        'value': abs(quantity * price)
    }
    
    log_business_event('trading', trading_data, logger)

def log_risk_event(risk_type: str, severity: str, details: Dict[str, Any], 
                  logger: logging.Logger):
    """è®°å½•é£æ§äº‹ä»¶"""
    risk_data = {
        'risk_type': risk_type,
        'severity': severity,
        'details': details
    }
    
    log_business_event('risk', risk_data, logger)
```

## ğŸš¨ å‘Šè­¦æœºåˆ¶

### 1. å‘Šè­¦é€šçŸ¥é…ç½®

#### é’‰é’‰æœºå™¨äººé›†æˆ

```python
# utils/alert_handlers.py
import requests
import json
from typing import Dict, List
import logging

class DingTalkNotifier:
    """é’‰é’‰é€šçŸ¥å™¨"""
    
    def __init__(self, webhook_url: str, secret: str = None):
        self.webhook_url = webhook_url
        self.secret = secret
        self.logger = logging.getLogger(__name__)
        
    def send_message(self, title: str, content: str, at_mobiles: List[str] = None):
        """å‘é€é’‰é’‰æ¶ˆæ¯"""
        try:
            payload = {
                "msgtype": "markdown",
                "markdown": {
                    "title": title,
                    "text": content
                }
            }
            
            if at_mobiles:
                payload["at"] = {
                    "atMobiles": at_mobiles,
                    "isAtAll": False
                }
                
            # å¦‚æœæœ‰å¯†é’¥ï¼Œè®¡ç®—ç­¾å
            if self.secret:
                import time
                import hmac
                import hashlib
                import base64
                from urllib.parse import quote_plus
                
                timestamp = str(round(time.time() * 1000))
                secret_enc = self.secret.encode('utf-8')
                string_to_sign = f'{timestamp}\n{self.secret}'
                string_to_sign_enc = string_to_sign.encode('utf-8')
                hmac_code = hmac.new(secret_enc, string_to_sign_enc, digestmod=hashlib.sha256).digest()
                sign = quote_plus(base64.b64encode(hmac_code))
                
                url = f"{self.webhook_url}&timestamp={timestamp}&sign={sign}"
            else:
                url = self.webhook_url
                
            response = requests.post(url, json=payload, timeout=10)
            response.raise_for_status()
            
            self.logger.info(f"é’‰é’‰æ¶ˆæ¯å‘é€æˆåŠŸ: {title}")
            
        except Exception as e:
            self.logger.error(f"é’‰é’‰æ¶ˆæ¯å‘é€å¤±è´¥: {e}")

class AlertManager:
    """å‘Šè­¦ç®¡ç†å™¨"""
    
    def __init__(self):
        self.notifiers = {}
        self.alert_rules = {}
        self.logger = logging.getLogger(__name__)
        
    def add_notifier(self, name: str, notifier):
        """æ·»åŠ é€šçŸ¥å™¨"""
        self.notifiers[name] = notifier
        
    def add_rule(self, rule_name: str, condition: callable, 
                 severity: str, notifiers: List[str]):
        """æ·»åŠ å‘Šè­¦è§„åˆ™"""
        self.alert_rules[rule_name] = {
            'condition': condition,
            'severity': severity,
            'notifiers': notifiers,
            'last_triggered': None
        }
        
    def check_alerts(self, metrics: Dict[str, float]):
        """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
        current_time = datetime.now()
        
        for rule_name, rule in self.alert_rules.items():
            try:
                if rule['condition'](metrics):
                    # é¿å…é‡å¤å‘Šè­¦ï¼ˆ5åˆ†é’Ÿå†…ï¼‰
                    if (rule['last_triggered'] and 
                        (current_time - rule['last_triggered']).seconds < 300):
                        continue
                        
                    self._trigger_alert(rule_name, rule, metrics)
                    rule['last_triggered'] = current_time
                    
            except Exception as e:
                self.logger.error(f"å‘Šè­¦è§„åˆ™æ£€æŸ¥å¤±è´¥ {rule_name}: {e}")
                
    def _trigger_alert(self, rule_name: str, rule: Dict, metrics: Dict):
        """è§¦å‘å‘Šè­¦"""
        severity = rule['severity']
        message = self._format_alert_message(rule_name, severity, metrics)
        
        # å‘é€é€šçŸ¥
        for notifier_name in rule['notifiers']:
            if notifier_name in self.notifiers:
                try:
                    notifier = self.notifiers[notifier_name]
                    if hasattr(notifier, 'send_message'):
                        notifier.send_message(
                            f"ã€{severity.upper()}ã€‘{rule_name}",
                            message
                        )
                except Exception as e:
                    self.logger.error(f"å‘é€å‘Šè­¦é€šçŸ¥å¤±è´¥ {notifier_name}: {e}")
                    
    def _format_alert_message(self, rule_name: str, severity: str, 
                            metrics: Dict) -> str:
        """æ ¼å¼åŒ–å‘Šè­¦æ¶ˆæ¯"""
        return f"""
### ğŸš¨ ç³»ç»Ÿå‘Šè­¦

**å‘Šè­¦è§„åˆ™**: {rule_name}  
**ä¸¥é‡çº§åˆ«**: {severity}  
**è§¦å‘æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  

**å½“å‰æŒ‡æ ‡**:  
{chr(10).join([f"- {k}: {v}" for k, v in metrics.items()])}

**å¤„ç†å»ºè®®**:  
è¯·ç«‹å³æ£€æŸ¥ç³»ç»ŸçŠ¶æ€å¹¶é‡‡å–ç›¸åº”æªæ–½ã€‚
        """

# ç¤ºä¾‹å‘Šè­¦è§„åˆ™é…ç½®
def setup_alert_rules():
    """è®¾ç½®å‘Šè­¦è§„åˆ™"""
    alert_manager = AlertManager()
    
    # æ·»åŠ é’‰é’‰é€šçŸ¥å™¨
    dingtalk = DingTalkNotifier(
        webhook_url="https://oapi.dingtalk.com/robot/send?access_token=xxx",
        secret="your_secret"
    )
    alert_manager.add_notifier('dingtalk', dingtalk)
    
    # CPUä½¿ç”¨ç‡å‘Šè­¦
    alert_manager.add_rule(
        'high_cpu_usage',
        lambda m: m.get('cpu_percent', 0) > 80,
        'warning',
        ['dingtalk']
    )
    
    # å†…å­˜ä½¿ç”¨ç‡å‘Šè­¦
    alert_manager.add_rule(
        'high_memory_usage',
        lambda m: m.get('memory_percent', 0) > 85,
        'critical',
        ['dingtalk']
    )
    
    # åº”ç”¨å“åº”æ—¶é—´å‘Šè­¦
    alert_manager.add_rule(
        'slow_response',
        lambda m: m.get('response_time_p95', 0) > 5,
        'warning',
        ['dingtalk']
    )
    
    # ç­–ç•¥å›æ’¤å‘Šè­¦
    alert_manager.add_rule(
        'high_drawdown',
        lambda m: m.get('portfolio_drawdown', 0) < -0.15,
        'critical',
        ['dingtalk']
    )
    
    return alert_manager
```

### 2. å‘Šè­¦å¤„ç†æµç¨‹

#### å‘Šè­¦å‡çº§æœºåˆ¶

```python
# utils/alert_escalation.py
from datetime import datetime, timedelta
from enum import Enum
from typing import Dict, List
import logging

class AlertSeverity(Enum):
    INFO = 1
    WARNING = 2
    CRITICAL = 3
    EMERGENCY = 4

class AlertStatus(Enum):
    OPEN = "open"
    ACKNOWLEDGED = "acknowledged"
    RESOLVED = "resolved"

class Alert:
    """å‘Šè­¦å¯¹è±¡"""
    
    def __init__(self, alert_id: str, rule_name: str, severity: AlertSeverity,
                 message: str, metrics: Dict):
        self.alert_id = alert_id
        self.rule_name = rule_name
        self.severity = severity
        self.message = message
        self.metrics = metrics
        self.status = AlertStatus.OPEN
        self.created_at = datetime.now()
        self.acknowledged_at = None
        self.resolved_at = None
        self.escalation_level = 0
        
    def acknowledge(self):
        """ç¡®è®¤å‘Šè­¦"""
        self.status = AlertStatus.ACKNOWLEDGED
        self.acknowledged_at = datetime.now()
        
    def resolve(self):
        """è§£å†³å‘Šè­¦"""
        self.status = AlertStatus.RESOLVED
        self.resolved_at = datetime.now()

class AlertEscalationManager:
    """å‘Šè­¦å‡çº§ç®¡ç†å™¨"""
    
    def __init__(self):
        self.active_alerts: Dict[str, Alert] = {}
        self.escalation_rules = {}
        self.on_call_schedule = {}
        self.logger = logging.getLogger(__name__)
        
    def add_escalation_rule(self, severity: AlertSeverity, escalation_steps: List[Dict]):
        """æ·»åŠ å‡çº§è§„åˆ™
        
        Args:
            severity: å‘Šè­¦çº§åˆ«
            escalation_steps: å‡çº§æ­¥éª¤åˆ—è¡¨
                [
                    {'delay_minutes': 5, 'contacts': ['team@company.com']},
                    {'delay_minutes': 15, 'contacts': ['manager@company.com']},
                    {'delay_minutes': 30, 'contacts': ['director@company.com']}
                ]
        """
        self.escalation_rules[severity] = escalation_steps
        
    def process_alert(self, alert: Alert):
        """å¤„ç†æ–°å‘Šè­¦"""
        self.active_alerts[alert.alert_id] = alert
        
        # ç«‹å³å‘é€é¦–æ¬¡é€šçŸ¥
        self._send_initial_notification(alert)
        
        # å¯åŠ¨å‡çº§æµç¨‹
        self._start_escalation(alert)
        
    def _send_initial_notification(self, alert: Alert):
        """å‘é€åˆå§‹é€šçŸ¥"""
        contacts = self._get_primary_contacts(alert.severity)
        self._send_notification(alert, contacts, escalation_level=0)
        
    def _start_escalation(self, alert: Alert):
        """å¯åŠ¨å‡çº§æµç¨‹"""
        if alert.severity not in self.escalation_rules:
            return
            
        import threading
        
        def escalation_worker():
            escalation_steps = self.escalation_rules[alert.severity]
            
            for step_index, step in enumerate(escalation_steps):
                # ç­‰å¾…æŒ‡å®šæ—¶é—´
                import time
                time.sleep(step['delay_minutes'] * 60)
                
                # æ£€æŸ¥å‘Šè­¦æ˜¯å¦å·²è¢«ç¡®è®¤æˆ–è§£å†³
                current_alert = self.active_alerts.get(alert.alert_id)
                if not current_alert or current_alert.status != AlertStatus.OPEN:
                    break
                    
                # å‡çº§é€šçŸ¥
                current_alert.escalation_level = step_index + 1
                self._send_notification(current_alert, step['contacts'], 
                                      escalation_level=step_index + 1)
                
        thread = threading.Thread(target=escalation_worker)
        thread.daemon = True
        thread.start()
        
    def _get_primary_contacts(self, severity: AlertSeverity) -> List[str]:
        """è·å–ä¸»è¦è”ç³»äºº"""
        if severity == AlertSeverity.CRITICAL or severity == AlertSeverity.EMERGENCY:
            return ['oncall@company.com']
        else:
            return ['team@company.com']
            
    def _send_notification(self, alert: Alert, contacts: List[str], 
                          escalation_level: int):
        """å‘é€é€šçŸ¥"""
        escalation_text = f" (å‡çº§ç­‰çº§: {escalation_level})" if escalation_level > 0 else ""
        
        message = f"""
ğŸš¨ ç³»ç»Ÿå‘Šè­¦{escalation_text}

å‘Šè­¦ID: {alert.alert_id}
è§„åˆ™åç§°: {alert.rule_name}
ä¸¥é‡ç¨‹åº¦: {alert.severity.name}
åˆ›å»ºæ—¶é—´: {alert.created_at.strftime('%Y-%m-%d %H:%M:%S')}
çŠ¶æ€: {alert.status.value}

è¯¦ç»†ä¿¡æ¯:
{alert.message}

å½“å‰æŒ‡æ ‡:
{self._format_metrics(alert.metrics)}

è¯·åŠæ—¶å¤„ç†æ­¤å‘Šè­¦ã€‚
        """
        
        # è¿™é‡Œå®ç°å®é™…çš„é€šçŸ¥å‘é€é€»è¾‘
        self.logger.info(f"å‘é€å‘Šè­¦é€šçŸ¥åˆ°: {contacts}")
        print(message)  # ç¤ºä¾‹è¾“å‡º
        
    def _format_metrics(self, metrics: Dict) -> str:
        """æ ¼å¼åŒ–æŒ‡æ ‡ä¿¡æ¯"""
        return '\n'.join([f"- {k}: {v}" for k, v in metrics.items()])
        
    def acknowledge_alert(self, alert_id: str, acknowledged_by: str):
        """ç¡®è®¤å‘Šè­¦"""
        if alert_id in self.active_alerts:
            alert = self.active_alerts[alert_id]
            alert.acknowledge()
            
            self.logger.info(f"å‘Šè­¦ {alert_id} è¢« {acknowledged_by} ç¡®è®¤")
            
    def resolve_alert(self, alert_id: str, resolved_by: str):
        """è§£å†³å‘Šè­¦"""
        if alert_id in self.active_alerts:
            alert = self.active_alerts[alert_id]
            alert.resolve()
            
            # ä»æ´»è·ƒå‘Šè­¦ä¸­ç§»é™¤
            del self.active_alerts[alert_id]
            
            self.logger.info(f"å‘Šè­¦ {alert_id} è¢« {resolved_by} è§£å†³")
            
    def get_active_alerts(self) -> List[Alert]:
        """è·å–æ´»è·ƒå‘Šè­¦åˆ—è¡¨"""
        return list(self.active_alerts.values())
```

## ğŸ“ˆ æ€§èƒ½ç›‘æ§

### 1. åº”ç”¨æ€§èƒ½ç›‘æ§

#### APMé›†æˆ

```python
# monitor/apm.py
import time
import psutil
import threading
from contextlib import contextmanager
from typing import Dict, Any
from collections import defaultdict, deque
import statistics

class PerformanceTracker:
    """æ€§èƒ½è·Ÿè¸ªå™¨"""
    
    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        self.metrics = defaultdict(lambda: deque(maxlen=window_size))
        self.counters = defaultdict(int)
        self.lock = threading.Lock()
        
    @contextmanager
    def track_execution_time(self, operation_name: str):
        """è·Ÿè¸ªæ‰§è¡Œæ—¶é—´çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        start_time = time.time()
        try:
            yield
        finally:
            end_time = time.time()
            execution_time = end_time - start_time
            
            with self.lock:
                self.metrics[f"{operation_name}_duration"].append(execution_time)
                self.counters[f"{operation_name}_count"] += 1
                
    def track_value(self, metric_name: str, value: float):
        """è·Ÿè¸ªæ•°å€¼æŒ‡æ ‡"""
        with self.lock:
            self.metrics[metric_name].append(value)
            
    def increment_counter(self, counter_name: str, amount: int = 1):
        """å¢åŠ è®¡æ•°å™¨"""
        with self.lock:
            self.counters[counter_name] += amount
            
    def get_statistics(self, metric_name: str) -> Dict[str, float]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            values = list(self.metrics[metric_name])
            
        if not values:
            return {}
            
        return {
            'count': len(values),
            'mean': statistics.mean(values),
            'median': statistics.median(values),
            'min': min(values),
            'max': max(values),
            'std_dev': statistics.stdev(values) if len(values) > 1 else 0,
            'p95': statistics.quantiles(values, n=20)[18] if len(values) >= 20 else max(values),
            'p99': statistics.quantiles(values, n=100)[98] if len(values) >= 100 else max(values)
        }
        
    def get_all_metrics(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰æŒ‡æ ‡"""
        result = {}
        
        # ç»Ÿè®¡æŒ‡æ ‡
        for metric_name in self.metrics:
            result[metric_name] = self.get_statistics(metric_name)
            
        # è®¡æ•°å™¨
        with self.lock:
            result['counters'] = dict(self.counters)
            
        return result

# å…¨å±€æ€§èƒ½è·Ÿè¸ªå™¨
performance_tracker = PerformanceTracker()

def monitor_performance(operation_name: str):
    """æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            with performance_tracker.track_execution_time(operation_name):
                return func(*args, **kwargs)
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@monitor_performance("data_fetch")
def fetch_stock_data(symbol: str):
    """è·å–è‚¡ç¥¨æ•°æ®"""
    # æ¨¡æ‹Ÿæ•°æ®è·å–
    time.sleep(0.1)
    return {"symbol": symbol, "price": 100.0}

@monitor_performance("strategy_calculation")
def calculate_strategy_signals(data):
    """è®¡ç®—ç­–ç•¥ä¿¡å·"""
    # æ¨¡æ‹Ÿç­–ç•¥è®¡ç®—
    time.sleep(0.05)
    return {"signal": "buy"}
```

### 2. æ•°æ®åº“æ€§èƒ½ç›‘æ§

```python
# monitor/db_monitor.py
import time
import logging
from typing import Dict, List
from contextlib import contextmanager
from sqlalchemy import event
from sqlalchemy.engine import Engine

class DatabaseMonitor:
    """æ•°æ®åº“æ€§èƒ½ç›‘æ§"""
    
    def __init__(self):
        self.slow_query_threshold = 1.0  # æ…¢æŸ¥è¯¢é˜ˆå€¼(ç§’)
        self.query_stats = defaultdict(list)
        self.logger = logging.getLogger(__name__)
        
    def setup_monitoring(self, engine: Engine):
        """è®¾ç½®æ•°æ®åº“ç›‘æ§"""
        
        @event.listens_for(engine, "before_cursor_execute")
        def receive_before_cursor_execute(conn, cursor, statement,
                                        parameters, context, executemany):
            context._query_start_time = time.time()
            
        @event.listens_for(engine, "after_cursor_execute")
        def receive_after_cursor_execute(conn, cursor, statement,
                                       parameters, context, executemany):
            total_time = time.time() - context._query_start_time
            
            # è®°å½•æŸ¥è¯¢ç»Ÿè®¡
            self._record_query_stats(statement, total_time)
            
            # è®°å½•æ…¢æŸ¥è¯¢
            if total_time > self.slow_query_threshold:
                self._log_slow_query(statement, total_time, parameters)
                
    def _record_query_stats(self, statement: str, execution_time: float):
        """è®°å½•æŸ¥è¯¢ç»Ÿè®¡"""
        # ç®€åŒ–SQLè¯­å¥ç”¨äºåˆ†ç»„
        simplified_query = self._simplify_query(statement)
        self.query_stats[simplified_query].append(execution_time)
        
        # æ›´æ–°PrometheusæŒ‡æ ‡
        from .metrics import performance_tracker
        performance_tracker.track_value('db_query_duration', execution_time)
        
    def _simplify_query(self, statement: str) -> str:
        """ç®€åŒ–SQLè¯­å¥"""
        # ç§»é™¤å‚æ•°å€¼ï¼Œä¿ç•™æŸ¥è¯¢æ¨¡å¼
        import re
        # æ›¿æ¢æ•°å­—å’Œå­—ç¬¦ä¸²å­—é¢é‡
        simplified = re.sub(r"'[^']*'", "'?'", statement)
        simplified = re.sub(r'\b\d+\b', '?', simplified)
        return simplified.strip()
        
    def _log_slow_query(self, statement: str, execution_time: float, 
                       parameters: Dict):
        """è®°å½•æ…¢æŸ¥è¯¢"""
        self.logger.warning(
            f"æ…¢æŸ¥è¯¢æ£€æµ‹: æ‰§è¡Œæ—¶é—´ {execution_time:.3f}s\n"
            f"SQL: {statement}\n"
            f"å‚æ•°: {parameters}"
        )
        
        # å‘é€å‘Šè­¦
        from .metrics import metrics_collector
        metrics_collector.track_value('slow_query_count', 1)
        
    def get_query_statistics(self) -> Dict[str, Dict]:
        """è·å–æŸ¥è¯¢ç»Ÿè®¡"""
        stats = {}
        
        for query, times in self.query_stats.items():
            if times:
                stats[query] = {
                    'count': len(times),
                    'avg_time': sum(times) / len(times),
                    'max_time': max(times),
                    'min_time': min(times),
                    'total_time': sum(times)
                }
                
        return stats
        
    def get_top_slow_queries(self, limit: int = 10) -> List[Dict]:
        """è·å–æœ€æ…¢æŸ¥è¯¢åˆ—è¡¨"""
        query_stats = self.get_query_statistics()
        
        # æŒ‰å¹³å‡æ‰§è¡Œæ—¶é—´æ’åº
        sorted_queries = sorted(
            query_stats.items(),
            key=lambda x: x[1]['avg_time'],
            reverse=True
        )
        
        return [
            {
                'query': query,
                'stats': stats
            }
            for query, stats in sorted_queries[:limit]
        ]

# å…¨å±€æ•°æ®åº“ç›‘æ§å™¨
db_monitor = DatabaseMonitor()
```

## ğŸ”„ æŒç»­ç›‘æ§

### 1. å¥åº·æ£€æŸ¥

```python
# monitor/health_check.py
from typing import Dict, List, Optional
from datetime import datetime
import asyncio
import aiohttp
import psutil
import logging

class HealthChecker:
    """å¥åº·æ£€æŸ¥å™¨"""
    
    def __init__(self):
        self.checks = {}
        self.logger = logging.getLogger(__name__)
        
    def register_check(self, name: str, check_func: callable, 
                      critical: bool = False, timeout: int = 30):
        """æ³¨å†Œå¥åº·æ£€æŸ¥"""
        self.checks[name] = {
            'func': check_func,
            'critical': critical,
            'timeout': timeout,
            'last_result': None,
            'last_check': None
        }
        
    async def run_all_checks(self) -> Dict[str, Dict]:
        """è¿è¡Œæ‰€æœ‰å¥åº·æ£€æŸ¥"""
        results = {}
        
        for name, check_config in self.checks.items():
            try:
                result = await asyncio.wait_for(
                    self._run_single_check(check_config['func']),
                    timeout=check_config['timeout']
                )
                
                results[name] = {
                    'status': 'healthy' if result['healthy'] else 'unhealthy',
                    'message': result.get('message', ''),
                    'details': result.get('details', {}),
                    'critical': check_config['critical'],
                    'timestamp': datetime.now().isoformat()
                }
                
                # æ›´æ–°ç¼“å­˜
                check_config['last_result'] = results[name]
                check_config['last_check'] = datetime.now()
                
            except asyncio.TimeoutError:
                results[name] = {
                    'status': 'timeout',
                    'message': f'å¥åº·æ£€æŸ¥è¶…æ—¶ ({check_config["timeout"]}s)',
                    'critical': check_config['critical'],
                    'timestamp': datetime.now().isoformat()
                }
            except Exception as e:
                results[name] = {
                    'status': 'error',
                    'message': f'å¥åº·æ£€æŸ¥å¼‚å¸¸: {str(e)}',
                    'critical': check_config['critical'],
                    'timestamp': datetime.now().isoformat()
                }
                
        return results
        
    async def _run_single_check(self, check_func):
        """è¿è¡Œå•ä¸ªå¥åº·æ£€æŸ¥"""
        if asyncio.iscoroutinefunction(check_func):
            return await check_func()
        else:
            return check_func()
            
    def get_overall_status(self, check_results: Dict) -> str:
        """è·å–æ•´ä½“å¥åº·çŠ¶æ€"""
        for check_name, result in check_results.items():
            if result['critical'] and result['status'] != 'healthy':
                return 'critical'
                
        # æ£€æŸ¥æ˜¯å¦æœ‰éå…³é”®æœåŠ¡ä¸å¥åº·
        unhealthy_count = sum(1 for r in check_results.values() 
                            if r['status'] != 'healthy')
        
        if unhealthy_count == 0:
            return 'healthy'
        elif unhealthy_count <= len(check_results) * 0.2:  # 20%ä»¥ä¸‹
            return 'degraded'
        else:
            return 'unhealthy'

# å…·ä½“å¥åº·æ£€æŸ¥å®ç°
async def check_database_health():
    """æ£€æŸ¥æ•°æ®åº“å¥åº·çŠ¶æ€"""
    try:
        from data.database import get_database_manager
        db = get_database_manager()
        
        # æ‰§è¡Œç®€å•æŸ¥è¯¢
        result = db.execute("SELECT 1")
        
        return {
            'healthy': True,
            'message': 'æ•°æ®åº“è¿æ¥æ­£å¸¸',
            'details': {'query_result': result}
        }
    except Exception as e:
        return {
            'healthy': False,
            'message': f'æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}'
        }

async def check_redis_health():
    """æ£€æŸ¥Rediså¥åº·çŠ¶æ€"""
    try:
        import redis
        r = redis.Redis(host='localhost', port=6379, db=0)
        
        # æ‰§è¡Œpingå‘½ä»¤
        result = r.ping()
        
        return {
            'healthy': result,
            'message': 'Redisè¿æ¥æ­£å¸¸' if result else 'Redis pingå¤±è´¥'
        }
    except Exception as e:
        return {
            'healthy': False,
            'message': f'Redisè¿æ¥å¤±è´¥: {str(e)}'
        }

async def check_external_api_health():
    """æ£€æŸ¥å¤–éƒ¨APIå¥åº·çŠ¶æ€"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get('https://api.tushare.pro/health', 
                                 timeout=10) as response:
                if response.status == 200:
                    return {
                        'healthy': True,
                        'message': 'å¤–éƒ¨APIå¯è®¿é—®',
                        'details': {'status_code': response.status}
                    }
                else:
                    return {
                        'healthy': False,
                        'message': f'å¤–éƒ¨APIè¿”å›é”™è¯¯çŠ¶æ€: {response.status}'
                    }
    except Exception as e:
        return {
            'healthy': False,
            'message': f'å¤–éƒ¨APIæ£€æŸ¥å¤±è´¥: {str(e)}'
        }

def check_system_resources():
    """æ£€æŸ¥ç³»ç»Ÿèµ„æº"""
    try:
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        issues = []
        if cpu_percent > 90:
            issues.append(f'CPUä½¿ç”¨ç‡è¿‡é«˜: {cpu_percent:.1f}%')
        if memory.percent > 90:
            issues.append(f'å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory.percent:.1f}%')
        if disk.percent > 90:
            issues.append(f'ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜: {disk.percent:.1f}%')
            
        return {
            'healthy': len(issues) == 0,
            'message': 'ç³»ç»Ÿèµ„æºæ­£å¸¸' if not issues else '; '.join(issues),
            'details': {
                'cpu_percent': cpu_percent,
                'memory_percent': memory.percent,
                'disk_percent': disk.percent
            }
        }
    except Exception as e:
        return {
            'healthy': False,
            'message': f'ç³»ç»Ÿèµ„æºæ£€æŸ¥å¤±è´¥: {str(e)}'
        }

# è®¾ç½®å¥åº·æ£€æŸ¥
def setup_health_checks():
    """è®¾ç½®å¥åº·æ£€æŸ¥"""
    health_checker = HealthChecker()
    
    # æ³¨å†Œå…³é”®æœåŠ¡æ£€æŸ¥
    health_checker.register_check('database', check_database_health, critical=True)
    health_checker.register_check('redis', check_redis_health, critical=True)
    health_checker.register_check('system', check_system_resources, critical=True)
    
    # æ³¨å†Œéå…³é”®æœåŠ¡æ£€æŸ¥
    health_checker.register_check('external_api', check_external_api_health, critical=False)
    
    return health_checker
```

### 2. è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬

```bash
#!/bin/bash
# scripts/monitor_check.sh - è‡ªåŠ¨åŒ–ç›‘æ§æ£€æŸ¥è„šæœ¬

set -e

LOG_FILE="/var/log/lianghua/monitor_check.log"
ALERT_WEBHOOK="http://localhost:5000/webhook/monitoring"

log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a "$LOG_FILE"
}

check_service_status() {
    local service_name=$1
    if systemctl is-active --quiet "$service_name"; then
        log "âœ“ $service_name æœåŠ¡è¿è¡Œæ­£å¸¸"
        return 0
    else
        log "âœ— $service_name æœåŠ¡å¼‚å¸¸"
        return 1
    fi
}

check_application_health() {
    local url="http://localhost:5000/health"
    local response=$(curl -s -o /dev/null -w "%{http_code}" "$url" --max-time 10)
    
    if [ "$response" = "200" ]; then
        log "âœ“ åº”ç”¨å¥åº·æ£€æŸ¥é€šè¿‡"
        return 0
    else
        log "âœ— åº”ç”¨å¥åº·æ£€æŸ¥å¤±è´¥ (HTTP $response)"
        return 1
    fi
}

check_disk_space() {
    local threshold=90
    local usage=$(df / | awk 'NR==2 {print $5}' | sed 's/%//')
    
    if [ "$usage" -lt "$threshold" ]; then
        log "âœ“ ç£ç›˜ç©ºé—´æ­£å¸¸ (ä½¿ç”¨ç‡: ${usage}%)"
        return 0
    else
        log "âœ— ç£ç›˜ç©ºé—´ä¸è¶³ (ä½¿ç”¨ç‡: ${usage}%)"
        return 1
    fi
}

check_memory_usage() {
    local threshold=90
    local usage=$(free | grep Mem | awk '{printf "%.0f", $3/$2 * 100}')
    
    if [ "$usage" -lt "$threshold" ]; then
        log "âœ“ å†…å­˜ä½¿ç”¨æ­£å¸¸ (ä½¿ç”¨ç‡: ${usage}%)"
        return 0
    else
        log "âœ— å†…å­˜ä½¿ç”¨è¿‡é«˜ (ä½¿ç”¨ç‡: ${usage}%)"
        return 1
    fi
}

check_log_errors() {
    local error_count=$(grep -c "ERROR" /var/log/lianghua/app.log 2>/dev/null || echo 0)
    local threshold=10
    
    if [ "$error_count" -lt "$threshold" ]; then
        log "âœ“ åº”ç”¨é”™è¯¯æ—¥å¿—æ­£å¸¸ (é”™è¯¯æ•°: $error_count)"
        return 0
    else
        log "âœ— åº”ç”¨é”™è¯¯æ—¥å¿—è¿‡å¤š (é”™è¯¯æ•°: $error_count)"
        return 1
    fi
}

send_alert() {
    local message="$1"
    local severity="$2"
    
    curl -X POST "$ALERT_WEBHOOK" \
         -H "Content-Type: application/json" \
         -d "{\"message\": \"$message\", \"severity\": \"$severity\"}" \
         --max-time 10 || log "å‘Šè­¦å‘é€å¤±è´¥"
}

main() {
    log "å¼€å§‹ç³»ç»Ÿç›‘æ§æ£€æŸ¥..."
    
    local failed_checks=0
    local check_results=()
    
    # æ‰§è¡Œæ£€æŸ¥
    services=("mysql" "redis" "nginx")
    for service in "${services[@]}"; do
        if ! check_service_status "$service"; then
            failed_checks=$((failed_checks + 1))
            check_results+=("$service æœåŠ¡å¼‚å¸¸")
        fi
    done
    
    if ! check_application_health; then
        failed_checks=$((failed_checks + 1))
        check_results+=("åº”ç”¨å¥åº·æ£€æŸ¥å¤±è´¥")
    fi
    
    if ! check_disk_space; then
        failed_checks=$((failed_checks + 1))
        check_results+=("ç£ç›˜ç©ºé—´ä¸è¶³")
    fi
    
    if ! check_memory_usage; then
        failed_checks=$((failed_checks + 1))
        check_results+=("å†…å­˜ä½¿ç”¨è¿‡é«˜")
    fi
    
    if ! check_log_errors; then
        failed_checks=$((failed_checks + 1))
        check_results+=("åº”ç”¨é”™è¯¯è¿‡å¤š")
    fi
    
    # æ±‡æ€»ç»“æœ
    if [ "$failed_checks" -eq 0 ]; then
        log "âœ“ æ‰€æœ‰æ£€æŸ¥é¡¹é€šè¿‡"
    else
        log "âœ— $failed_checks ä¸ªæ£€æŸ¥é¡¹å¤±è´¥"
        
        # å‘é€å‘Šè­¦
        alert_message="ç³»ç»Ÿç›‘æ§æ£€æŸ¥å¤±è´¥é¡¹: $(IFS=', '; echo "${check_results[*]}")"
        send_alert "$alert_message" "warning"
    fi
    
    log "ç›‘æ§æ£€æŸ¥å®Œæˆ"
}

# è¿è¡Œæ£€æŸ¥
main "$@"
```

## ğŸ“± è¿ç»´é¢æ¿

### 1. Webç›‘æ§é¢æ¿

```python
# monitor/admin_dashboard.py
from flask import Blueprint, render_template, jsonify, request
from flask_login import login_required
import asyncio
from .health_check import setup_health_checks
from .metrics import performance_tracker
from .db_monitor import db_monitor

admin_bp = Blueprint('admin', __name__, url_prefix='/admin')

@admin_bp.route('/dashboard')
@login_required
def dashboard():
    """ç›‘æ§ä»ªè¡¨æ¿ä¸»é¡µ"""
    return render_template('admin/dashboard.html')

@admin_bp.route('/api/system/health')
@login_required
def system_health():
    """ç³»ç»Ÿå¥åº·çŠ¶æ€API"""
    health_checker = setup_health_checks()
    
    # è¿è¡Œå¥åº·æ£€æŸ¥
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    try:
        check_results = loop.run_until_complete(health_checker.run_all_checks())
        overall_status = health_checker.get_overall_status(check_results)
        
        return jsonify({
            'status': overall_status,
            'checks': check_results,
            'timestamp': datetime.now().isoformat()
        })
    finally:
        loop.close()

@admin_bp.route('/api/metrics/performance')
@login_required
def performance_metrics():
    """æ€§èƒ½æŒ‡æ ‡API"""
    metrics = performance_tracker.get_all_metrics()
    
    return jsonify({
        'metrics': metrics,
        'timestamp': datetime.now().isoformat()
    })

@admin_bp.route('/api/database/stats')
@login_required
def database_stats():
    """æ•°æ®åº“ç»Ÿè®¡API"""
    query_stats = db_monitor.get_query_statistics()
    slow_queries = db_monitor.get_top_slow_queries()
    
    return jsonify({
        'query_statistics': query_stats,
        'slow_queries': slow_queries,
        'timestamp': datetime.now().isoformat()
    })

@admin_bp.route('/api/alerts/active')
@login_required
def active_alerts():
    """æ´»è·ƒå‘Šè­¦API"""
    # ä»å‘Šè­¦ç®¡ç†å™¨è·å–æ´»è·ƒå‘Šè­¦
    from .alert_handlers import alert_manager
    alerts = alert_manager.get_active_alerts()
    
    return jsonify({
        'alerts': [
            {
                'id': alert.alert_id,
                'rule_name': alert.rule_name,
                'severity': alert.severity.name,
                'status': alert.status.value,
                'created_at': alert.created_at.isoformat(),
                'message': alert.message
            }
            for alert in alerts
        ],
        'count': len(alerts)
    })

@admin_bp.route('/api/alerts/<alert_id>/acknowledge', methods=['POST'])
@login_required
def acknowledge_alert(alert_id):
    """ç¡®è®¤å‘Šè­¦"""
    from .alert_handlers import alert_manager
    from flask_login import current_user
    
    alert_manager.acknowledge_alert(alert_id, current_user.username)
    
    return jsonify({'status': 'success', 'message': 'å‘Šè­¦å·²ç¡®è®¤'})

@admin_bp.route('/api/system/restart', methods=['POST'])
@login_required
def restart_system():
    """é‡å¯ç³»ç»ŸæœåŠ¡"""
    service = request.json.get('service')
    
    if service not in ['app', 'redis', 'nginx']:
        return jsonify({'status': 'error', 'message': 'æ— æ•ˆçš„æœåŠ¡åç§°'}), 400
    
    try:
        import subprocess
        if service == 'app':
            # é‡å¯åº”ç”¨
            subprocess.run(['sudo', 'systemctl', 'restart', 'lianghua_vn'], check=True)
        else:
            subprocess.run(['sudo', 'systemctl', 'restart', service], check=True)
            
        return jsonify({'status': 'success', 'message': f'{service} æœåŠ¡é‡å¯æˆåŠŸ'})
    except subprocess.CalledProcessError as e:
        return jsonify({'status': 'error', 'message': f'æœåŠ¡é‡å¯å¤±è´¥: {str(e)}'}), 500
```

### 2. è¿ç»´å·¥å…·è„šæœ¬

```python
# scripts/ops_tools.py
#!/usr/bin/env python3
"""è¿ç»´å·¥å…·è„šæœ¬"""

import click
import json
import requests
import pandas as pd
from datetime import datetime, timedelta
import matplotlib.pyplot as plt

@click.group()
def cli():
    """é‡åŒ–äº¤æ˜“ç³»ç»Ÿè¿ç»´å·¥å…·"""
    pass

@cli.command()
@click.option('--days', default=7, help='æŸ¥è¯¢å¤©æ•°')
def performance_report(days):
    """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days)
    
    click.echo(f"ç”Ÿæˆ {start_date.date()} åˆ° {end_date.date()} çš„æ€§èƒ½æŠ¥å‘Š...")
    
    # è·å–æ€§èƒ½æ•°æ®
    try:
        response = requests.get('http://localhost:5000/api/metrics/performance')
        response.raise_for_status()
        data = response.json()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'period': f"{start_date.date()} to {end_date.date()}",
            'summary': {
                'total_requests': data['metrics']['counters'].get('http_requests_count', 0),
                'avg_response_time': data['metrics'].get('http_request_duration', {}).get('mean', 0),
                'error_rate': calculate_error_rate(data['metrics'])
            },
            'timestamp': datetime.now().isoformat()
        }
        
        # è¾“å‡ºæŠ¥å‘Š
        click.echo(json.dumps(report, indent=2, ensure_ascii=False))
        
    except Exception as e:
        click.echo(f"è·å–æ€§èƒ½æ•°æ®å¤±è´¥: {e}", err=True)

@cli.command()
@click.option('--service', required=True, help='æœåŠ¡åç§°')
def restart_service(service):
    """é‡å¯æœåŠ¡"""
    click.echo(f"é‡å¯æœåŠ¡: {service}")
    
    try:
        response = requests.post(
            'http://localhost:5000/api/system/restart',
            json={'service': service},
            headers={'Authorization': 'Bearer your_token'}
        )
        response.raise_for_status()
        result = response.json()
        
        if result['status'] == 'success':
            click.echo(f"âœ“ {result['message']}")
        else:
            click.echo(f"âœ— {result['message']}", err=True)
            
    except Exception as e:
        click.echo(f"é‡å¯æœåŠ¡å¤±è´¥: {e}", err=True)

@cli.command()
def health_check():
    """æ‰§è¡Œå¥åº·æ£€æŸ¥"""
    click.echo("æ‰§è¡Œç³»ç»Ÿå¥åº·æ£€æŸ¥...")
    
    try:
        response = requests.get('http://localhost:5000/api/system/health')
        response.raise_for_status()
        data = response.json()
        
        click.echo(f"æ•´ä½“çŠ¶æ€: {data['status']}")
        click.echo("\næ£€æŸ¥è¯¦æƒ…:")
        
        for check_name, result in data['checks'].items():
            status_icon = "âœ“" if result['status'] == 'healthy' else "âœ—"
            critical_mark = " [å…³é”®]" if result['critical'] else ""
            
            click.echo(f"  {status_icon} {check_name}{critical_mark}: {result['message']}")
            
    except Exception as e:
        click.echo(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}", err=True)

@cli.command()
@click.option('--output', default='alerts.json', help='è¾“å‡ºæ–‡ä»¶')
def export_alerts(output):
    """å¯¼å‡ºå‘Šè­¦ä¿¡æ¯"""
    click.echo("å¯¼å‡ºå‘Šè­¦ä¿¡æ¯...")
    
    try:
        response = requests.get('http://localhost:5000/api/alerts/active')
        response.raise_for_status()
        data = response.json()
        
        with open(output, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
            
        click.echo(f"å‘Šè­¦ä¿¡æ¯å·²å¯¼å‡ºåˆ° {output}")
        click.echo(f"æ´»è·ƒå‘Šè­¦æ•°é‡: {data['count']}")
        
    except Exception as e:
        click.echo(f"å¯¼å‡ºå‘Šè­¦å¤±è´¥: {e}", err=True)

@cli.command()
@click.option('--period', default='1h', help='æŸ¥è¯¢å‘¨æœŸ (1h, 6h, 1d, 7d)')
def system_metrics(period):
    """æŸ¥çœ‹ç³»ç»ŸæŒ‡æ ‡"""
    click.echo(f"æŸ¥è¯¢ {period} çš„ç³»ç»ŸæŒ‡æ ‡...")
    
    # è¿™é‡Œåº”è¯¥è°ƒç”¨Prometheus APIè·å–æŒ‡æ ‡
    # ç®€åŒ–ç¤ºä¾‹
    metrics = {
        'cpu_usage': 45.2,
        'memory_usage': 67.8,
        'disk_usage': 34.1,
        'network_in': '1.2MB/s',
        'network_out': '0.8MB/s'
    }
    
    click.echo("\nç³»ç»ŸæŒ‡æ ‡:")
    for metric, value in metrics.items():
        click.echo(f"  {metric}: {value}")

def calculate_error_rate(metrics):
    """è®¡ç®—é”™è¯¯ç‡"""
    total_requests = metrics['counters'].get('http_requests_count', 0)
    error_requests = metrics['counters'].get('http_errors_count', 0)
    
    if total_requests > 0:
        return error_requests / total_requests
    return 0

if __name__ == '__main__':
    cli()
```

## ğŸ“‹ è¿ç»´æ£€æŸ¥æ¸…å•

### æ—¥å¸¸æ£€æŸ¥é¡¹ç›®

#### æ¯æ—¥æ£€æŸ¥ (Daily Checklist)
- [ ] ç³»ç»Ÿå¥åº·çŠ¶æ€æ£€æŸ¥
- [ ] åº”ç”¨æœåŠ¡è¿è¡ŒçŠ¶æ€
- [ ] æ•°æ®åº“è¿æ¥å’Œæ€§èƒ½
- [ ] ç£ç›˜ç©ºé—´ä½¿ç”¨æƒ…å†µ
- [ ] å†…å­˜å’ŒCPUä½¿ç”¨ç‡
- [ ] ç½‘ç»œè¿æ¥çŠ¶æ€
- [ ] å¤‡ä»½ä½œä¸šæ‰§è¡Œæƒ…å†µ
- [ ] å‘Šè­¦æ¶ˆæ¯å¤„ç†
- [ ] äº¤æ˜“æ•°æ®å®Œæ•´æ€§
- [ ] ç­–ç•¥è¿è¡ŒçŠ¶æ€

#### å‘¨åº¦æ£€æŸ¥ (Weekly Checklist)
- [ ] æ€§èƒ½è¶‹åŠ¿åˆ†æ
- [ ] æ…¢æŸ¥è¯¢ä¼˜åŒ–
- [ ] æ—¥å¿—è½®è½¬æ£€æŸ¥
- [ ] å®‰å…¨æ›´æ–°æ£€æŸ¥
- [ ] å¤‡ä»½æ¢å¤æµ‹è¯•
- [ ] ç›‘æ§è§„åˆ™è°ƒä¼˜
- [ ] å®¹é‡è§„åˆ’è¯„ä¼°
- [ ] æ•…éšœå¤„ç†å›é¡¾

#### æœˆåº¦æ£€æŸ¥ (Monthly Checklist)
- [ ] ç³»ç»Ÿå®¹é‡è¯„ä¼°
- [ ] æ€§èƒ½åŸºçº¿æ›´æ–°
- [ ] ç¾å¤‡æ¼”ç»ƒ
- [ ] å®‰å…¨å®¡è®¡
- [ ] æˆæœ¬ä¼˜åŒ–åˆ†æ
- [ ] è¿ç»´æµç¨‹æ”¹è¿›
- [ ] æ–‡æ¡£æ›´æ–°ç»´æŠ¤
- [ ] å›¢é˜ŸæŠ€èƒ½åŸ¹è®­

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025å¹´8æœˆ18æ—¥  
**ç»´æŠ¤å›¢é˜Ÿ**: è¿ç»´å›¢é˜Ÿ