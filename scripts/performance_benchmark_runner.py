"""
æ€§èƒ½åŸºå‡†æµ‹è¯•è¿è¡Œå™¨

è‡ªåŠ¨åŒ–è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•å¹¶ç”ŸæˆæŠ¥å‘Š
"""

import asyncio
import logging
import json
import time
import statistics
from typing import Dict, Any, List, Optional, Callable, Tuple
from datetime import datetime, timedelta
import sys
import os
import concurrent.futures
from dataclasses import dataclass
import threading

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from validation import BenchmarkRunner, PerformanceAnalyzer
from optimization import DatabaseOptimizer, CacheManager

logger = logging.getLogger(__name__)


@dataclass
class BenchmarkScenario:
    """åŸºå‡†æµ‹è¯•åœºæ™¯"""
    name: str
    description: str
    category: str
    duration_seconds: int
    concurrent_users: int
    operations_per_user: int
    parameters: Dict[str, Any]
    expected_metrics: Dict[str, float]


class PerformanceBenchmarkResult:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ"""
    
    def __init__(self, scenario_name: str):
        self.scenario_name = scenario_name
        self.start_time = datetime.now()
        self.end_time = None
        self.duration_seconds = 0
        
        # æ€§èƒ½æŒ‡æ ‡
        self.throughput = 0.0  # ååé‡ (ops/sec)
        self.response_times = []  # å“åº”æ—¶é—´åˆ—è¡¨
        self.success_count = 0
        self.failure_count = 0
        self.error_details = []
        
        # ç»Ÿè®¡æŒ‡æ ‡
        self.avg_response_time = 0.0
        self.p50_response_time = 0.0
        self.p90_response_time = 0.0
        self.p95_response_time = 0.0
        self.p99_response_time = 0.0
        self.min_response_time = 0.0
        self.max_response_time = 0.0
        
        # èµ„æºä½¿ç”¨
        self.cpu_usage = []
        self.memory_usage = []
        self.network_io = []
        self.disk_io = []
        
        # è¯„ä¼°ç»“æœ
        self.performance_score = 0.0
        self.bottlenecks = []
        self.recommendations = []
        self.status = "running"
    
    def finalize(self):
        """å®Œæˆæµ‹è¯•å¹¶è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡"""
        self.end_time = datetime.now()
        self.duration_seconds = (self.end_time - self.start_time).total_seconds()
        
        if self.response_times:
            self.avg_response_time = statistics.mean(self.response_times)
            self.min_response_time = min(self.response_times)
            self.max_response_time = max(self.response_times)
            
            sorted_times = sorted(self.response_times)
            n = len(sorted_times)
            
            self.p50_response_time = sorted_times[int(n * 0.5)]
            self.p90_response_time = sorted_times[int(n * 0.9)]
            self.p95_response_time = sorted_times[int(n * 0.95)]
            self.p99_response_time = sorted_times[int(n * 0.99)]
        
        # è®¡ç®—ååé‡
        total_operations = self.success_count + self.failure_count
        if self.duration_seconds > 0:
            self.throughput = total_operations / self.duration_seconds
        
        # è®¡ç®—æˆåŠŸç‡
        self.success_rate = self.success_count / max(total_operations, 1)
        
        # è¯„ä¼°æ€§èƒ½
        self._evaluate_performance()
        
        self.status = "completed"
    
    def _evaluate_performance(self):
        """è¯„ä¼°æ€§èƒ½"""
        score = 100.0
        
        # åŸºäºå“åº”æ—¶é—´è¯„åˆ†
        if self.avg_response_time > 2.0:
            score -= 30
        elif self.avg_response_time > 1.0:
            score -= 15
        elif self.avg_response_time > 0.5:
            score -= 5
        
        # åŸºäºæˆåŠŸç‡è¯„åˆ†
        if self.success_rate < 0.95:
            score -= 40
        elif self.success_rate < 0.98:
            score -= 20
        elif self.success_rate < 0.99:
            score -= 10
        
        # åŸºäºP95å“åº”æ—¶é—´è¯„åˆ†
        if self.p95_response_time > 5.0:
            score -= 20
        elif self.p95_response_time > 3.0:
            score -= 10
        
        self.performance_score = max(score, 0)
        
        # ç”Ÿæˆç“¶é¢ˆåˆ†æ
        if self.avg_response_time > 1.0:
            self.bottlenecks.append("å¹³å‡å“åº”æ—¶é—´è¿‡é«˜")
        
        if self.success_rate < 0.99:
            self.bottlenecks.append("æ“ä½œæˆåŠŸç‡åä½")
        
        if self.p95_response_time > 3.0:
            self.bottlenecks.append("P95å“åº”æ—¶é—´è¿‡é«˜")
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        if self.avg_response_time > 1.0:
            self.recommendations.append("ä¼˜åŒ–åº”ç”¨æ€§èƒ½ï¼Œå‡å°‘å“åº”æ—¶é—´")
        
        if self.success_rate < 0.99:
            self.recommendations.append("æé«˜ç³»ç»Ÿç¨³å®šæ€§ï¼Œå‡å°‘é”™è¯¯ç‡")
        
        if self.throughput < 100:
            self.recommendations.append("æå‡ç³»ç»Ÿååé‡")
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'scenario_name': self.scenario_name,
            'start_time': self.start_time.isoformat(),
            'end_time': self.end_time.isoformat() if self.end_time else None,
            'duration_seconds': self.duration_seconds,
            'throughput': self.throughput,
            'success_count': self.success_count,
            'failure_count': self.failure_count,
            'success_rate': getattr(self, 'success_rate', 0),
            'response_time_stats': {
                'avg': self.avg_response_time,
                'min': self.min_response_time,
                'max': self.max_response_time,
                'p50': self.p50_response_time,
                'p90': self.p90_response_time,
                'p95': self.p95_response_time,
                'p99': self.p99_response_time
            },
            'performance_score': self.performance_score,
            'bottlenecks': self.bottlenecks,
            'recommendations': self.recommendations,
            'error_details': self.error_details[:10],  # åªä¿å­˜å‰10ä¸ªé”™è¯¯
            'status': self.status
        }


class PerformanceBenchmarkRunner:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
        # åŸºå‡†æµ‹è¯•é…ç½®
        self.benchmark_config = {
            'default_duration': 60,  # é»˜è®¤æµ‹è¯•æ—¶é•¿ï¼ˆç§’ï¼‰
            'default_concurrent_users': 10,  # é»˜è®¤å¹¶å‘ç”¨æˆ·æ•°
            'default_operations_per_user': 100,  # é»˜è®¤æ¯ç”¨æˆ·æ“ä½œæ•°
            'warmup_duration': 10,  # é¢„çƒ­æ—¶é—´ï¼ˆç§’ï¼‰
            'cooldown_duration': 5,  # å†·å´æ—¶é—´ï¼ˆç§’ï¼‰
            'monitor_resources': True,  # æ˜¯å¦ç›‘æ§èµ„æºä½¿ç”¨
            'save_results': True,  # æ˜¯å¦ä¿å­˜ç»“æœ
            'results_directory': 'benchmark_results',
            'scenarios': self._get_default_scenarios(),
            'performance_thresholds': {
                'avg_response_time': 1.0,
                'p95_response_time': 2.0,
                'success_rate': 0.99,
                'throughput': 100
            }
        }
        
        # æ›´æ–°é…ç½®
        if 'performance_benchmark' in config:
            self.benchmark_config.update(config['performance_benchmark'])
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.benchmark_runner = BenchmarkRunner(config)
        self.performance_analyzer = PerformanceAnalyzer(config)
        
        # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
        os.makedirs(self.benchmark_config['results_directory'], exist_ok=True)
        
        # ç›‘æ§çŠ¶æ€
        self.monitoring_active = False
        self.resource_monitor_task = None
    
    def _get_default_scenarios(self) -> List[Dict[str, Any]]:
        """è·å–é»˜è®¤æµ‹è¯•åœºæ™¯"""
        return [
            {
                'name': 'data_processing_light',
                'description': 'è½»é‡çº§æ•°æ®å¤„ç†æµ‹è¯•',
                'category': 'data_processing',
                'duration_seconds': 30,
                'concurrent_users': 5,
                'operations_per_user': 50,
                'parameters': {'data_size': 'small'},
                'expected_metrics': {
                    'avg_response_time': 0.5,
                    'throughput': 50
                }
            },
            {
                'name': 'data_processing_heavy',
                'description': 'é‡é‡çº§æ•°æ®å¤„ç†æµ‹è¯•',
                'category': 'data_processing',
                'duration_seconds': 60,
                'concurrent_users': 10,
                'operations_per_user': 100,
                'parameters': {'data_size': 'large'},
                'expected_metrics': {
                    'avg_response_time': 1.0,
                    'throughput': 100
                }
            },
            {
                'name': 'trading_operations_normal',
                'description': 'æ­£å¸¸äº¤æ˜“æ“ä½œæµ‹è¯•',
                'category': 'trading',
                'duration_seconds': 45,
                'concurrent_users': 8,
                'operations_per_user': 75,
                'parameters': {'order_type': 'market'},
                'expected_metrics': {
                    'avg_response_time': 0.3,
                    'throughput': 200
                }
            },
            {
                'name': 'database_operations_crud',
                'description': 'CRUDæ“ä½œæµ‹è¯•',
                'category': 'database',
                'duration_seconds': 40,
                'concurrent_users': 15,
                'operations_per_user': 80,
                'parameters': {'operation_mix': 'crud'},
                'expected_metrics': {
                    'avg_response_time': 0.2,
                    'throughput': 300
                }
            },
            {
                'name': 'cache_operations_intensive',
                'description': 'ç¼“å­˜å¯†é›†æ“ä½œæµ‹è¯•',
                'category': 'cache',
                'duration_seconds': 30,
                'concurrent_users': 20,
                'operations_per_user': 100,
                'parameters': {'cache_hit_ratio': 0.8},
                'expected_metrics': {
                    'avg_response_time': 0.1,
                    'throughput': 500
                }
            }
        ]
    
    async def run_benchmark_suite(self, scenarios: Optional[List[str]] = None) -> Dict[str, PerformanceBenchmarkResult]:
        """è¿è¡ŒåŸºå‡†æµ‹è¯•å¥—ä»¶"""
        logger.info("å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶")
        
        results = {}
        
        try:
            # ç¡®å®šè¦è¿è¡Œçš„åœºæ™¯
            available_scenarios = self.benchmark_config['scenarios']
            
            if scenarios:
                # è¿‡æ»¤æŒ‡å®šçš„åœºæ™¯
                scenarios_to_run = [
                    s for s in available_scenarios 
                    if s['name'] in scenarios
                ]
            else:
                scenarios_to_run = available_scenarios
            
            if not scenarios_to_run:
                raise ValueError("æ²¡æœ‰æ‰¾åˆ°å¯è¿è¡Œçš„æµ‹è¯•åœºæ™¯")
            
            logger.info(f"å°†è¿è¡Œ {len(scenarios_to_run)} ä¸ªæµ‹è¯•åœºæ™¯")
            
            # å¯åŠ¨èµ„æºç›‘æ§
            if self.benchmark_config['monitor_resources']:
                await self._start_resource_monitoring()
            
            # ä¾æ¬¡è¿è¡Œæ¯ä¸ªåœºæ™¯
            for i, scenario_config in enumerate(scenarios_to_run):
                logger.info(f"è¿è¡Œåœºæ™¯ {i+1}/{len(scenarios_to_run)}: {scenario_config['name']}")
                
                scenario = BenchmarkScenario(**scenario_config)
                result = await self._run_single_scenario(scenario)
                results[scenario.name] = result
                
                # åœºæ™¯é—´ä¼‘æ¯
                if i < len(scenarios_to_run) - 1:
                    logger.info("åœºæ™¯é—´ä¼‘æ¯...")
                    await asyncio.sleep(self.benchmark_config['cooldown_duration'])
            
            # åœæ­¢èµ„æºç›‘æ§
            if self.benchmark_config['monitor_resources']:
                await self._stop_resource_monitoring()
            
            # ä¿å­˜ç»“æœ
            if self.benchmark_config['save_results']:
                await self._save_suite_results(results)
            
            logger.info(f"åŸºå‡†æµ‹è¯•å¥—ä»¶å®Œæˆï¼Œè¿è¡Œäº† {len(results)} ä¸ªåœºæ™¯")
            
        except Exception as e:
            logger.error(f"åŸºå‡†æµ‹è¯•å¥—ä»¶å¤±è´¥: {e}")
            if self.benchmark_config['monitor_resources']:
                await self._stop_resource_monitoring()
            raise
        
        return results
    
    async def _run_single_scenario(self, scenario: BenchmarkScenario) -> PerformanceBenchmarkResult:
        """è¿è¡Œå•ä¸ªæµ‹è¯•åœºæ™¯"""
        logger.info(f"å¼€å§‹åœºæ™¯: {scenario.name}")
        
        result = PerformanceBenchmarkResult(scenario.name)
        
        try:
            # é¢„çƒ­é˜¶æ®µ
            logger.info("é¢„çƒ­é˜¶æ®µ...")
            await self._warmup_phase(scenario)
            
            # ä¸»æµ‹è¯•é˜¶æ®µ
            logger.info("ä¸»æµ‹è¯•é˜¶æ®µ...")
            result.start_time = datetime.now()
            
            # å¯åŠ¨å¹¶å‘ä»»åŠ¡
            tasks = []
            for user_id in range(scenario.concurrent_users):
                task = asyncio.create_task(
                    self._run_user_operations(scenario, user_id, result)
                )
                tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆæˆ–è¶…æ—¶
            try:
                await asyncio.wait_for(
                    asyncio.gather(*tasks, return_exceptions=True),
                    timeout=scenario.duration_seconds + 30  # é¢å¤–30ç§’è¶…æ—¶ç¼“å†²
                )
            except asyncio.TimeoutError:
                logger.warning(f"åœºæ™¯ {scenario.name} æ‰§è¡Œè¶…æ—¶")
                for task in tasks:
                    if not task.done():
                        task.cancel()
            
            # å®Œæˆæµ‹è¯•
            result.finalize()
            
            logger.info(f"åœºæ™¯ {scenario.name} å®Œæˆï¼Œæ€§èƒ½åˆ†æ•°: {result.performance_score:.1f}")
            
        except Exception as e:
            logger.error(f"åœºæ™¯ {scenario.name} æ‰§è¡Œå¤±è´¥: {e}")
            result.status = "failed"
            result.error_details.append(str(e))
            result.finalize()
        
        return result
    
    async def _warmup_phase(self, scenario: BenchmarkScenario):
        """é¢„çƒ­é˜¶æ®µ"""
        warmup_duration = self.benchmark_config['warmup_duration']
        
        # è¿è¡Œå°‘é‡æ“ä½œè¿›è¡Œé¢„çƒ­
        warmup_users = min(scenario.concurrent_users, 3)
        warmup_ops = min(scenario.operations_per_user, 10)
        
        tasks = []
        for user_id in range(warmup_users):
            task = asyncio.create_task(
                self._run_warmup_operations(scenario, user_id, warmup_ops)
            )
            tasks.append(task)
        
        try:
            await asyncio.wait_for(
                asyncio.gather(*tasks, return_exceptions=True),
                timeout=warmup_duration
            )
        except asyncio.TimeoutError:
            logger.warning("é¢„çƒ­é˜¶æ®µè¶…æ—¶")
    
    async def _run_warmup_operations(self, scenario: BenchmarkScenario, 
                                   user_id: int, operations: int):
        """è¿è¡Œé¢„çƒ­æ“ä½œ"""
        try:
            for _ in range(operations):
                await self._execute_operation(scenario, user_id, warmup=True)
                await asyncio.sleep(0.1)  # é¢„çƒ­æœŸé—´æ…¢ä¸€ç‚¹
        except Exception as e:
            logger.debug(f"é¢„çƒ­æ“ä½œå¤±è´¥: {e}")
    
    async def _run_user_operations(self, scenario: BenchmarkScenario, 
                                 user_id: int, result: PerformanceBenchmarkResult):
        """è¿è¡Œç”¨æˆ·æ“ä½œ"""
        operations_completed = 0
        start_time = time.time()
        
        try:
            while (operations_completed < scenario.operations_per_user and
                   time.time() - start_time < scenario.duration_seconds):
                
                try:
                    operation_start = time.time()
                    await self._execute_operation(scenario, user_id)
                    operation_end = time.time()
                    
                    response_time = operation_end - operation_start
                    result.response_times.append(response_time)
                    result.success_count += 1
                    
                except Exception as e:
                    result.failure_count += 1
                    error_msg = f"User {user_id} operation {operations_completed}: {str(e)}"
                    result.error_details.append(error_msg)
                    logger.debug(error_msg)
                
                operations_completed += 1
                
                # ç®€å•çš„é€Ÿç‡æ§åˆ¶
                await asyncio.sleep(0.01)
                
        except Exception as e:
            logger.error(f"ç”¨æˆ· {user_id} æ“ä½œå¤±è´¥: {e}")
    
    async def _execute_operation(self, scenario: BenchmarkScenario, 
                               user_id: int, warmup: bool = False):
        """æ‰§è¡Œå…·ä½“æ“ä½œ"""
        category = scenario.category
        parameters = scenario.parameters
        
        if category == 'data_processing':
            await self._execute_data_processing_operation(parameters, user_id)
        elif category == 'trading':
            await self._execute_trading_operation(parameters, user_id)
        elif category == 'database':
            await self._execute_database_operation(parameters, user_id)
        elif category == 'cache':
            await self._execute_cache_operation(parameters, user_id)
        else:
            await self._execute_generic_operation(parameters, user_id)
    
    async def _execute_data_processing_operation(self, parameters: Dict[str, Any], user_id: int):
        """æ‰§è¡Œæ•°æ®å¤„ç†æ“ä½œ"""
        try:
            data_size = parameters.get('data_size', 'small')
            
            if data_size == 'small':
                # æ¨¡æ‹Ÿå°æ•°æ®é‡å¤„ç†
                await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿ100mså¤„ç†æ—¶é—´
                data = list(range(100))
                result = sum(x * x for x in data)
            elif data_size == 'large':
                # æ¨¡æ‹Ÿå¤§æ•°æ®é‡å¤„ç†
                await asyncio.sleep(0.3)  # æ¨¡æ‹Ÿ300mså¤„ç†æ—¶é—´
                data = list(range(1000))
                result = sum(x * x for x in data)
            
            # æ¨¡æ‹Ÿè°ƒç”¨æ•°æ®å¤„ç†åŸºå‡†æµ‹è¯•
            await self.benchmark_runner.run_data_processing_benchmark()
            
        except Exception as e:
            raise Exception(f"æ•°æ®å¤„ç†æ“ä½œå¤±è´¥: {e}")
    
    async def _execute_trading_operation(self, parameters: Dict[str, Any], user_id: int):
        """æ‰§è¡Œäº¤æ˜“æ“ä½œ"""
        try:
            order_type = parameters.get('order_type', 'market')
            
            # æ¨¡æ‹Ÿè®¢å•å¤„ç†
            if order_type == 'market':
                await asyncio.sleep(0.05)  # å¸‚ä»·å•è¾ƒå¿«
            else:
                await asyncio.sleep(0.1)   # é™ä»·å•è¾ƒæ…¢
            
            # æ¨¡æ‹Ÿè°ƒç”¨äº¤æ˜“åŸºå‡†æµ‹è¯•
            await self.benchmark_runner.run_trading_benchmark()
            
        except Exception as e:
            raise Exception(f"äº¤æ˜“æ“ä½œå¤±è´¥: {e}")
    
    async def _execute_database_operation(self, parameters: Dict[str, Any], user_id: int):
        """æ‰§è¡Œæ•°æ®åº“æ“ä½œ"""
        try:
            operation_mix = parameters.get('operation_mix', 'crud')
            
            # æ¨¡æ‹Ÿæ•°æ®åº“æ“ä½œ
            if operation_mix == 'crud':
                # éšæœºé€‰æ‹©CRUDæ“ä½œ
                import random
                operation = random.choice(['create', 'read', 'update', 'delete'])
                
                if operation == 'read':
                    await asyncio.sleep(0.02)  # è¯»å–è¾ƒå¿«
                else:
                    await asyncio.sleep(0.05)  # å†™å…¥è¾ƒæ…¢
            
            # æ¨¡æ‹Ÿè°ƒç”¨æ•°æ®åº“åŸºå‡†æµ‹è¯•
            await self.benchmark_runner.run_database_benchmark()
            
        except Exception as e:
            raise Exception(f"æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
    
    async def _execute_cache_operation(self, parameters: Dict[str, Any], user_id: int):
        """æ‰§è¡Œç¼“å­˜æ“ä½œ"""
        try:
            cache_hit_ratio = parameters.get('cache_hit_ratio', 0.8)
            
            # æ¨¡æ‹Ÿç¼“å­˜æ“ä½œ
            import random
            if random.random() < cache_hit_ratio:
                await asyncio.sleep(0.001)  # ç¼“å­˜å‘½ä¸­å¾ˆå¿«
            else:
                await asyncio.sleep(0.1)    # ç¼“å­˜æœªå‘½ä¸­éœ€è¦æŸ¥æ•°æ®åº“
            
            # æ¨¡æ‹Ÿè°ƒç”¨ç¼“å­˜åŸºå‡†æµ‹è¯•
            await self.benchmark_runner.run_cache_benchmark()
            
        except Exception as e:
            raise Exception(f"ç¼“å­˜æ“ä½œå¤±è´¥: {e}")
    
    async def _execute_generic_operation(self, parameters: Dict[str, Any], user_id: int):
        """æ‰§è¡Œé€šç”¨æ“ä½œ"""
        # é»˜è®¤æ“ä½œ
        await asyncio.sleep(0.05)
    
    async def _start_resource_monitoring(self):
        """å¯åŠ¨èµ„æºç›‘æ§"""
        try:
            self.monitoring_active = True
            await self.performance_analyzer.start_monitoring()
            
            # å¯åŠ¨èµ„æºç›‘æ§ä»»åŠ¡
            self.resource_monitor_task = asyncio.create_task(
                self._monitor_resources()
            )
            
            logger.info("èµ„æºç›‘æ§å·²å¯åŠ¨")
            
        except Exception as e:
            logger.error(f"å¯åŠ¨èµ„æºç›‘æ§å¤±è´¥: {e}")
    
    async def _stop_resource_monitoring(self):
        """åœæ­¢èµ„æºç›‘æ§"""
        try:
            self.monitoring_active = False
            
            if self.resource_monitor_task:
                self.resource_monitor_task.cancel()
                try:
                    await self.resource_monitor_task
                except asyncio.CancelledError:
                    pass
            
            await self.performance_analyzer.stop_monitoring()
            
            logger.info("èµ„æºç›‘æ§å·²åœæ­¢")
            
        except Exception as e:
            logger.error(f"åœæ­¢èµ„æºç›‘æ§å¤±è´¥: {e}")
    
    async def _monitor_resources(self):
        """ç›‘æ§èµ„æºä½¿ç”¨"""
        while self.monitoring_active:
            try:
                metrics = await self.performance_analyzer.get_current_metrics()
                
                # è®°å½•èµ„æºä½¿ç”¨æƒ…å†µï¼ˆè¿™é‡Œå¯ä»¥å­˜å‚¨åˆ°ç»“æœä¸­ï¼‰
                logger.debug(f"èµ„æºä½¿ç”¨: CPU={metrics.get('cpu_usage', 0):.1f}%, "
                           f"Memory={metrics.get('memory_usage', 0):.1f}%")
                
                await asyncio.sleep(1)  # æ¯ç§’ç›‘æ§ä¸€æ¬¡
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"èµ„æºç›‘æ§å¼‚å¸¸: {e}")
                await asyncio.sleep(5)
    
    async def _save_suite_results(self, results: Dict[str, PerformanceBenchmarkResult]):
        """ä¿å­˜å¥—ä»¶ç»“æœ"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"benchmark_suite_{timestamp}.json"
            filepath = os.path.join(self.benchmark_config['results_directory'], filename)
            
            # è½¬æ¢ç»“æœä¸ºå¯åºåˆ—åŒ–æ ¼å¼
            results_dict = {
                'timestamp': datetime.now().isoformat(),
                'suite_summary': self._generate_suite_summary(results),
                'scenarios': {name: result.to_dict() for name, result in results.items()}
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results_dict, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜æœ€æ–°ç»“æœ
            latest_filepath = os.path.join(
                self.benchmark_config['results_directory'], 
                "latest_benchmark_suite.json"
            )
            with open(latest_filepath, 'w', encoding='utf-8') as f:
                json.dump(results_dict, f, indent=2, ensure_ascii=False)
            
            logger.info(f"åŸºå‡†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœå¤±è´¥: {e}")
    
    def _generate_suite_summary(self, results: Dict[str, PerformanceBenchmarkResult]) -> Dict[str, Any]:
        """ç”Ÿæˆå¥—ä»¶æ‘˜è¦"""
        if not results:
            return {}
        
        total_scenarios = len(results)
        successful_scenarios = sum(1 for r in results.values() if r.status == "completed")
        failed_scenarios = total_scenarios - successful_scenarios
        
        # è®¡ç®—å¹³å‡æ€§èƒ½åˆ†æ•°
        completed_results = [r for r in results.values() if r.status == "completed"]
        avg_performance_score = 0
        if completed_results:
            avg_performance_score = statistics.mean(r.performance_score for r in completed_results)
        
        # æ”¶é›†æ‰€æœ‰ç“¶é¢ˆå’Œå»ºè®®
        all_bottlenecks = []
        all_recommendations = []
        for result in completed_results:
            all_bottlenecks.extend(result.bottlenecks)
            all_recommendations.extend(result.recommendations)
        
        # å»é‡å¹¶è®¡æ•°
        bottleneck_counts = {}
        for bottleneck in all_bottlenecks:
            bottleneck_counts[bottleneck] = bottleneck_counts.get(bottleneck, 0) + 1
        
        recommendation_counts = {}
        for recommendation in all_recommendations:
            recommendation_counts[recommendation] = recommendation_counts.get(recommendation, 0) + 1
        
        return {
            'total_scenarios': total_scenarios,
            'successful_scenarios': successful_scenarios,
            'failed_scenarios': failed_scenarios,
            'success_rate': successful_scenarios / total_scenarios,
            'avg_performance_score': avg_performance_score,
            'common_bottlenecks': sorted(bottleneck_counts.items(), key=lambda x: x[1], reverse=True)[:5],
            'common_recommendations': sorted(recommendation_counts.items(), key=lambda x: x[1], reverse=True)[:5],
            'overall_status': self._determine_overall_status(avg_performance_score, successful_scenarios, total_scenarios)
        }
    
    def _determine_overall_status(self, avg_score: float, successful: int, total: int) -> str:
        """ç¡®å®šæ€»ä½“çŠ¶æ€"""
        success_rate = successful / total
        
        if success_rate < 0.8:
            return "critical"
        elif success_rate < 0.9 or avg_score < 60:
            return "poor"
        elif avg_score < 80:
            return "acceptable"
        elif avg_score < 90:
            return "good"
        else:
            return "excellent"
    
    async def run_custom_scenario(self, scenario_config: Dict[str, Any]) -> PerformanceBenchmarkResult:
        """è¿è¡Œè‡ªå®šä¹‰åœºæ™¯"""
        logger.info(f"è¿è¡Œè‡ªå®šä¹‰åœºæ™¯: {scenario_config.get('name', 'unnamed')}")
        
        try:
            scenario = BenchmarkScenario(**scenario_config)
            
            if self.benchmark_config['monitor_resources']:
                await self._start_resource_monitoring()
            
            result = await self._run_single_scenario(scenario)
            
            if self.benchmark_config['monitor_resources']:
                await self._stop_resource_monitoring()
            
            return result
            
        except Exception as e:
            logger.error(f"è‡ªå®šä¹‰åœºæ™¯æ‰§è¡Œå¤±è´¥: {e}")
            if self.benchmark_config['monitor_resources']:
                await self._stop_resource_monitoring()
            raise
    
    def generate_performance_report(self, results: Dict[str, PerformanceBenchmarkResult]) -> str:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        if not results:
            return "æ²¡æœ‰å¯ç”¨çš„åŸºå‡†æµ‹è¯•ç»“æœ"
        
        report_lines = [
            "=" * 80,
            "æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š",
            "=" * 80,
            f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"æµ‹è¯•åœºæ™¯æ•°: {len(results)}",
            ""
        ]
        
        # æ€»ä½“æ‘˜è¦
        summary = self._generate_suite_summary(results)
        report_lines.extend([
            "æ€»ä½“æ‘˜è¦:",
            "-" * 40,
            f"æˆåŠŸåœºæ™¯: {summary.get('successful_scenarios', 0)}/{summary.get('total_scenarios', 0)}",
            f"å¹³å‡æ€§èƒ½åˆ†æ•°: {summary.get('avg_performance_score', 0):.1f}",
            f"æ€»ä½“çŠ¶æ€: {summary.get('overall_status', 'unknown').upper()}",
            ""
        ])
        
        # å„åœºæ™¯è¯¦æƒ…
        report_lines.extend([
            "åœºæ™¯è¯¦æƒ…:",
            "-" * 40
        ])
        
        for name, result in results.items():
            status_icon = "âœ“" if result.status == "completed" else "âœ—"
            score_color = "ğŸŸ¢" if result.performance_score >= 80 else "ğŸŸ¡" if result.performance_score >= 60 else "ğŸ”´"
            
            report_lines.extend([
                f"{status_icon} {name}",
                f"  æ€§èƒ½åˆ†æ•°: {score_color} {result.performance_score:.1f}/100",
                f"  ååé‡: {result.throughput:.1f} ops/sec",
                f"  å¹³å‡å“åº”æ—¶é—´: {result.avg_response_time:.3f}s",
                f"  P95å“åº”æ—¶é—´: {result.p95_response_time:.3f}s",
                f"  æˆåŠŸç‡: {getattr(result, 'success_rate', 0):.2%}",
                ""
            ])
            
            if result.bottlenecks:
                report_lines.extend([
                    "  ç“¶é¢ˆ:",
                    *[f"    â€¢ {bottleneck}" for bottleneck in result.bottlenecks[:3]],
                    ""
                ])
        
        # ä¼˜åŒ–å»ºè®®
        common_recommendations = summary.get('common_recommendations', [])
        if common_recommendations:
            report_lines.extend([
                "ä¼˜åŒ–å»ºè®®:",
                "-" * 40
            ])
            for recommendation, count in common_recommendations:
                report_lines.append(f"â€¢ {recommendation} (å‡ºç° {count} æ¬¡)")
            report_lines.append("")
        
        report_lines.append("=" * 80)
        
        return "\n".join(report_lines)


async def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ‰§è¡Œ"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ€§èƒ½åŸºå‡†æµ‹è¯•è¿è¡Œå™¨')
    parser.add_argument('--config', '-c', default='config.json', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--scenarios', '-s', nargs='+', help='æŒ‡å®šè¦è¿è¡Œçš„åœºæ™¯')
    parser.add_argument('--output', '-o', help='ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--report', '-r', action='store_true', help='ç”ŸæˆæŠ¥å‘Š')
    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO if args.verbose else logging.WARNING,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    try:
        # åŠ è½½é…ç½®
        if os.path.exists(args.config):
            with open(args.config, 'r', encoding='utf-8') as f:
                config = json.load(f)
        else:
            logger.warning(f"é…ç½®æ–‡ä»¶ {args.config} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            config = {}
        
        # åˆ›å»ºè¿è¡Œå™¨
        runner = PerformanceBenchmarkRunner(config)
        
        # è¿è¡ŒåŸºå‡†æµ‹è¯•
        results = await runner.run_benchmark_suite(args.scenarios)
        
        # ç”ŸæˆæŠ¥å‘Š
        if args.report:
            report = runner.generate_performance_report(results)
            print(report)
        else:
            # ç®€å•æ‘˜è¦
            summary = runner._generate_suite_summary(results)
            print(f"åŸºå‡†æµ‹è¯•å®Œæˆ:")
            print(f"  æˆåŠŸåœºæ™¯: {summary.get('successful_scenarios', 0)}/{summary.get('total_scenarios', 0)}")
            print(f"  å¹³å‡æ€§èƒ½åˆ†æ•°: {summary.get('avg_performance_score', 0):.1f}")
            print(f"  æ€»ä½“çŠ¶æ€: {summary.get('overall_status', 'unknown').upper()}")
        
        # ä¿å­˜ç»“æœåˆ°æŒ‡å®šæ–‡ä»¶
        if args.output:
            results_dict = {
                'timestamp': datetime.now().isoformat(),
                'scenarios': {name: result.to_dict() for name, result in results.items()}
            }
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(results_dict, f, indent=2, ensure_ascii=False)
            print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
        
        # æ ¹æ®æµ‹è¯•ç»“æœè®¾ç½®é€€å‡ºç 
        summary = runner._generate_suite_summary(results)
        if summary.get('overall_status') in ['critical', 'poor']:
            sys.exit(1)
        else:
            sys.exit(0)
            
    except Exception as e:
        logger.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        print(f"é”™è¯¯: {e}")
        sys.exit(1)


if __name__ == '__main__':
    asyncio.run(main())